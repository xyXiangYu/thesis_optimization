
As show above, both the classical approach and training approach leads to an identical solution with final time as $T=4.6053$, which is conservative with respect to original OCP \ref{rc}. The time $T=4.6053$ is conservative, regardless how the uncertain parameter $p$ takes a value in the uncertainty set $\mathbb{P}$, we can always find a feasible solution that leads to a final time that is less than or equal to $T=4.6053$. Neverthless, the classical approach and the training approach do not necessary lead to an identical solution for a general OCP under uncertainty. In general, classical approach should lead to a worse solution compared to training approach. In our case, it just happens that both approaches lead to the same solution, and the worst case happens when $p=9$ at the right boundary point.  

% the classcical approach and training apporach lead

%Before we discuss the results from the classic approach and training approach, we discuss our primary analysis of the original problem \ref{rc}. 
%We can further prove the theoretical solution in formulation \ref{eq_Theory_T} and \ref{eq_Theory_u} by showing the following facts. First, with more time steps
%When we plot the optimal $T$ against the uncertain parameter $p$, then we get the result in Figure  
%The implementation deatils and result are as follows. 
%\begin{figure}[h!]
%	\centerline{\includegraphics[width=10cm]{original_u10_p2.png}}
%	\caption{price and delta from three different methods}
%	\label{fig:result}
%\end{figure}
%\begin{figure}[h!]
%	\centerline{\includegraphics[width=10cm]{original_u10_p7.png}}
%	\caption{price and delta from three different methods}
%	\label{fig:result}
%\end{figure}

The optimal objective function value of the $max\  min$ problem (training approach) over estimates the one of the $min\ max$ problem (classical approach). It is easy to find examples in which the gap is indeed greater than zero. For example, let $\Omega_x=[-5,5],\Omega_p=[-1,1]$ and consider the function

In theory, the classical approach will lead to a solution that is worse than the training approach. Within training approach, the $p$ value is known beforehand and worst solution is obtained by maximizing over all $p \in \mathbb{P}$. While in the classical approach the $p$ value is priori unknown, and we need to find solutions that yield parameter-dependent variables for all possible realizations of the uncertain parameters, and among them find the optimal solution. We do not dive deep into the proof (ref \cite{MatSch22} for details), here we show the concusion in Theorem \ref{Theorem_compare} directly and one example to support the idea. 

%In the classical approach, when finding the solution of the lower level part for each $p$, we can find a feasiable solution $u(:,p), x(:,p)$ for each $p$. However, if we force a unified $u(t)$ for all $p$ in the uncertainty set, we cannot find a fesiable $u(t)$ for all the $p$ values if the uncertainty set is too big.  When we narrow the uncertainty set, for example, if we narrow the uncertainty set to $\mathbb{P}=[0,1]$, we can still find fesiable solution, with shared control states (i.e. $u(t)$) for all the parameters within this uncertainty set $\mathbb{P}=[0,1]$, as shown in Figure \ref{fig1_ca_unifiedUt}. In Figure  \ref{fig1_ca_unifiedUt}, we have plot the $x$ and the unified/shared $u(t)$ for the parameter at the boundary $p=0$ and $p=1$. Clearly, as shown in the subplot for $x_1:$ Position and $x_2:$ Velocity, both trajectories satisfiy the constraints with the unified $u(t)$ in the third subplot. However, if we increase the uncertainty set, we fail to find a shared $u(t)$ which leads to fesiable solutions for all $p$ in $\mathbb{P}=[0,9]$.  This is consistent with the theoretical analysis discussed in the next section \ref{Comparison}, that the classical approach will lead to a solution that is not better than the training approach. 

%In this part, we show that our numerical implementation is stable with different initial values $u(t_0) \in [0,10]$. As explained before, the car should accelerate as fast as possible at the begining, i.e. $u(t_0)$ should take maximum allowable value $10$ at the begining in order to have the optimal/smallest $T$. With different $u(t_0) \in [0,10]$, the control variables $u(t)$ moves towards $10$ at the acceleration stage, as shown in the first subplot of Figure \ref{fig1_initialUt}. In this plot, we have chosen only $100$ subintervals for $[0,1]$. As shown in the second subplot of Figure \ref{fig1_initialUt}, when the initial value of $u(t_0)$ deviates with the ideal initial value $u(t_0)=10$, it takes most time $T$, compared to other initial values. When the number of subintervals is increased, then the $T$ with initial value other than $u(t_0)=10$ will converge to the $T^\star(u(t_0)=10)$. The result in this section again confirms with the background knowledge discussed in this paper and the theorectical solution. 



%From analyzing the theoretical solution, 
%The solution of the training approach in this paper is, therefore, also a function of the realizations of the uncertain parameters. Since in the 
%The numerical result will be shown in the next section \ref{Sec_NR}.

%Numerical result to be added for the classical approach.  

%where $p_i$ is one of the discretized point $p_0, p_1, ..., p_n$  from the  $\mathbb{P}$. Therefore, for each $ p_i \in \mathbb{P}$, we get a corresponding solution. In the end, we get a robustified parameter-dependent varaibles for realizations of the unceretain parameters. The bigger the number $n$, i.e. the more points of $p_i$, the more precise that the obtained solution can represent the whole uncertain set. In this classical approach, the driver has no prior knowledge about the value of the parameter $p$ and gets no feedback during the process and has to set up the driving strategy in advance. The numerical implementation result of the classical approach is shown in section \ref{Sec_NR} when comparing that of the training approach. 

% \ \forall \   \hat{p}_i,
%As stated in the introduction part, the classical approach is consistent with the $minmax$ approach, during which, two level optimization problems are solved. In the lower level, we solve an optimization problem ($max \  f(x,p)$) with respect to $p$, and in the upper level, we continue to find the best solution with respect to $x$, as shown in \ref{minmax}. In the case of the rocket car, the classical approach will be expressed in the following form

%In the classical approach, the set of feasible controllable parameters and control functions are given by those $T$ and $u(\cdot)$, which yield feasible trajectories $x(\cdot, p)$ for all $p \in \mathbb{P}$. The value of the objective function in the lower level does not depend on $p$ and $x(\cdot, p)$. In other words, in this approach, the driver has no prior knowledge about the value of the parameter $p$ and gets no feedback during the process and has to set up the driving strategy in advance.  

% \section{The Training Approach}


We have explained that the trajectories of different $p_i$ from the classical approach may not be optimal, compared to the case when solving the OCP \ref{rc} directly with the known $p_i$. In the following, we compare the trajectories of the result from the classical approach for $p_i=0$, $p_i=5$, and $p_i=9$ to the cases when solving the OCP \ref{rc} directly with $p_i=0$, $p_i=5$, and $p_i=9$ known in advance. The results from solving OCP \ref{rc} directly with $p_i=0$, $p_i=5$, and $p_i=9$ known in advance, are already shown in Figures \ref{fig1_org_u10_p0},  \ref{fig1_org_u10_p5} and \ref{fig1_org_u10_p9}. 

In Figure \ref{fig_ca_compare_p0}, Figure \ref{fig_ca_compare_p5} and Figure \ref{fig_ca_compare_p9}, we show the comparisons for $p_i=0$, $p_i=5$, and $p_i=9$ between results of solving OCP \ref{rc} directly and results applying classical approach to problem \ref{ca_rc}. Obviously, we can see from from Figure \ref{fig_ca_compare_p0} and Figure \ref{fig_ca_compare_p5}, the solution from classical apporoach is more conservative compared to solving OCP \ref{rc} directly for $p=0$ and $p=5$ respectively. When $p=9$, there is only one line in Figure \ref{fig_ca_compare_p9} for the position $x_1(t)$, force $u(t)$ and velocity $x_2(t)$. This confirms that indeed, when $p=9$, the worst case happens, and the corresponding $\epsilon=4.6053$ is equal to result of solving the \ref{rc} directly with $p=9$. For other $p_i \neq 9$, we can always find an feasible solution with a corresponding $T_i$, which will always be less than or equal to $4.6053$. 

%Here, we take the result again  We take the result from solving OCP directly with with $p_i=0$, $p_i=5$, and $p_i=9$ known and the results from classical approach with $p_i=0$, $p_i=5$, and $p_i=9$, and compare the results. The comparsions for  $p_i=0$, $p_i=5$, and $p_i=9$ are shown in Figure \ref{fig_ca_compare_p0}, Figure \ref{fig_ca_compare_p5} and Figure \ref{fig_ca_compare_p9} respectively.
%and we get t and $\epsilon$ which are conservative to all the $p_i \in \mathbb{P}$, and does not depend on any parameter $p$. 




For each $(p_i, x(t;p_i), (t;p_i))$, we get one $T_i$, which we try to get maximum value, and meanwhile over the $T_i, i =0, 1, ..., n$, we would like to minimize $\epsilon$ ($\epsilon = \underset{i}{max} \ T_i$). In the end, we reach a worst $\epsilon$ over all $T_i, i =0, 1, ..., n$, and for each $i$, we get a trajectory $(p_i, x(t;p_i), (t;p_i))$. 

In the actual implementation, we have take $n=40$, i.e. in total $40 \ p_i$ points ranging the whole uncertainty set $\mathbb{P}=[0,9]$. Nevertheless, we only show the result for $p_i=0, 1, 2, 3, 4, 5, 6, 7, 8, 9$ for the sake of readability. The final time $\epsilon$ and $T_i$  is shown in Figure \ref{fig_ca_result}, where $\epsilon = \underset{i}{max} \ T_i, i = 0, 1, 2, ..., 9$, and $\epsilon$ reaches the maximum value $4.6053$, when $p_i=9$. The differences in $T_i$ arise mainly because of the numerical errors, in our implementation, the number of subinterval is chosen as $500$ and the numerical error tolerance level is set $1e-6$. When we increase the number of subintervals and decrease the tolerance level, the $T_i$ of $p_i\neq 9$ should converge to $T_i$ of $p_i=9$. 


The trajectories $(p_i, x(t;p_i), (t;p_i))$ for $p_i=0, 1, 2, 3, 4, 5, 6, 7, 8, 9$ are shown in Figure \ref{fig_ca_ut_pis}, Figure \ref{fig_ca_st_pis} and Figure \ref{fig_ca_vt_pis} respectively. In all the three Figures \ref{fig_ca_ut_pis}, \ref{fig_ca_st_pis} and \ref{fig_ca_vt_pis}, the standardized time $t \in [0,1]$ is used as x-axis. In Figure \ref{fig_ca_ut_pis}, the $u(t)$ acceleration/deceleration Force is shown for each $p_i=0, 1, 2, 3, 4, 5, 6, 7, 8, 9$; in Figure \ref{fig_ca_st_pis}, the position ($x_1(t)$) is shown for each $p_i=0, 1, 2, 3, 4, 5, 6, 7, 8, 9$; and in Figure \ref{fig_ca_vt_pis}, the velocity ($x_2(t)$) is shown for each $p_i=0, 1, 2, 3, 4, 5, 6, 7, 8, 9$. 

We have explained in Section \ref{Sec:CA} about the classical appraoch. Within this section, we show the numerical results of applying the classical approach for the chosen rocket car case. 

First we discretize the whole uncertainty set  $\mathbb{P}$ into multiple points in an increasing order $[p_0, p_1, ..., p_n] \subset \mathbb{P}$ so that $p_i, i =0, 1, ..., n$ can approximately representing the whole set $\mathbb{P}$ when $n$ is large enough. Here $p_0=p_l$ and $p_n= p_u$, with $[p_l, p_u] =[0,9]$, as shown in equation \ref{uncertainP}.  We solve the $minmax$ problem of the following form


% Applying the classical approach to the rocket car case, the problem can be solved as follows. 

%Because of the simplicity of the rocket car case, we can find the theoretical solution of the nominal/original problem for our case \ref{rc}. But for many real life problems, it is very difficult to find a direct solution to the original problem, and for some cases not feasible, due to the uncertainty in the parameter $p$. That is why in the paper \cite{MatSch22}, a classical (in the form $minmax$) approach and a training (in the form $maxmin$) approach have been discussed, and both approaches will lead to a conservative solution to the original problem. A conservative solution to the problem in the paper \cite{MatSch22}, or to general problems,  is a acceptable (or desired) result since less risk is taken and the result is more robust. 

%For both the classical approach and the training approach, after the discretization of the uncertainty set, the problem to be solved becomes a normal optimal control problem and we can use the mutiple shooting and quasi Newton (in the SQP framework) method to solve. We are levaraging open source package Casadi\footnote{Please refer the website https://web.casadi.org for more details} and Gekko\footnote{Please refer the website https://gekko.readthedocs.io/en/latest/ for more information} to solve our problem. These two package can let the user choose different nonlinear programming solver and as well as different underlying numerical methods. In our case, we have chosen the mutiple shooting and sequential quadratic programming, together with Runge–Kutta RK4 method. We have also discretized $t \in [0,1]$ into $500$ subintervals $\mathbb{I}_j = [\tau_{j-1}, \tau_j]$, i.e. the discretizion points as $0 = t_0 =  < t_1 < ... < t_m = t_f =1 $, and an error tolerance level as $1e-6$. The numerical result from the classical approach and training approach will be discussed in the next two sections. 


%In the next chapter, we discuss, in details, the classical approach and training approach for the rocket car problem. %After that, we focus on the quasi-Newton and multi shooting approach to the same problem.  

% By ananlyzing the optimal strHere we discuss several strategies. First assume at the ve
% and the ending state must be at a position larger than $10$ and velocity smaller than $0$. To reach the maximum velocity, from the starting state, we should accelerate as fast as possible until we reach the velocity of $x_2 =4$, and then keep the car running at this velocity until we need to decelerate as strongly as possible. The moment we start to decelerate is decided by how fast we can decelerate the velocity decreases to exact $0$ is just the moment that the position reaches
%The proof is very tedious, but the idea is straightforward.

%When $p$ takes fixed values, the OCP under uncertainty becomes a normal OCP problem and we can solve directly with the numerical methods discussed in Chapter \ref{Chapter2}, and no bilevel optimisation is needed. With $[0,1]$ discretized into $500$ subintervals $\mathbb{I}_j = [\tau_{j-1}, \tau_j]$, i.e. the discretizion points as $0 = t_0 =  < t_1 < ... < t_m = t_f =1 $, and an error tolerance level as $1e-6$, and Runge-Kutta RK4 method used,  the result from solving a normal OCP with $p=0$, $p=5$ and $p=9$ are shown in Figure \ref{fig1_org_u10_p0}, Figure \ref{fig1_org_u10_p5} and Figure \ref{fig1_org_u10_p9} respectively. 

%Based on these three Figures \ref{fig1_org_u10_p0},  \ref{fig1_org_u10_p5} and \ref{fig1_org_u10_p9}, the results from $p=0$,  $p=5$,  and $p=9$ are consistent with that from the theoretical results in formulation \ref{eq_Theory_T} and \ref{eq_Theory_u}, as shown in Figure \ref{theory_T_diff_p} and Figure \ref{theory_ut_3p}. This confirms two points: first, the theoretical results are correct and second, the numerical methods we have chosen can solve our rocket car case with one chosen fixed $p$ value. These methods can, therefore, be used to solve general OCPs.

%We can verify the correctness of the theoretical solution numerically as well. Firstly, we demonstrate the behavior of the theoretical solution for $T$ as $p$ varies over the entire uncertainty set $\mathbb{P}=[0,9]$, as depicted in Figure \ref{theory_T_diff_p}. Then, we eliminate the uncertainty by presenting the solution to the original problem \ref{rc} for several selected $p_i$ values, where $p_i \in [p_0, p_1, ..., p_n] \subset \mathbb{P} = [0,9]$. For illustrative purposes, we have chosen $p=0$, $p=5$, and $p=9$ as examples. The theoretical $T$ values and corresponding $u(t)$ values for these three $p$ values are demonstrated in Figure \ref{theory_ut_3p}.


%We can also numerically verified that the theoretical solution is correct. First, we show the theoretical solution of $T$ against different $p$ values accross the whole uncertainty set $\subset \mathbb{P} = [0,9]$, as shown in Figure \ref{theory_T_diff_p}. Then, we remove the uncertainty by showing the solution to the original problem \ref{rc} for several chosen $p_i$ values, where $ p_i  \in [p_0, p_1, ..., p_n] \subset \mathbb{P} = [0,9]$. Here, we have chosen $p=0$,  $p=5$,  and $p=9$ as the example for illustration.  The $T$ values and the theoretical  $u(t)$ values corresponding to $p=0$,  $p=5$,  and $p=9$, are shown in Figure \ref{theory_ut_3p}. 

This is a qualitative explanation why the solution from formulation \ref{eq_Theory_T} and \ref{eq_Theory_u} is correct. Now, we show that with $p=0$, the optimal time is indeed $T=2.9$. At the beginning $\tau_0 = 0$,  we should accelerate as much as possible until the real time $\tau_1=0.4$ (i.e. $t= \frac{\tau}{T} = 0.137931$). This number $\tau_1=0.4$ can be obtained by solving the partial differential equation \ref{rc_partial} with boundary conditions $x_2(\tau_0)=0$ and $x_2(\tau_1)=4$. At $\tau_1=0.4$, the car velocity reaches $4$, and it keeps running at this velocity $x_2=4$ till $\tau_2=2.5$. From $\tau=2.5$, the car starts to decelerate as strongly as possible, and at $T=\tau_3=2.9$, the velocity reaches $0$ and the total distance traveled is exactly $10$. Then all the contraints are satisfied, and the optimal time is $T=2.9$. 

We can prove that $T=2.9$ is indeed the optimal solution when $p=0$ by using the method proof by contradiction. In the optimal strategy above, it has three stages $[\tau_0, \tau_1]$, $[\tau_1, \tau_2]$, and $[\tau_2, \tau_3]$, corresponding to acceleration, constant and deceleration stages. We assume there is another strategy that differs the one we have described above, yet leading to a less time while satisfying all the constraints. First, we assume the alternative strategy does not accelerate as strong as possible before $\tau_1$, this means within this alternative strategy, the velocity of the car is always less or equal to the velocity of our optimal strategy before $\tau_1=0.4$. This means that before $\tau_1=0.4$, the travel distance in the alternative strategy is less than our optimal strategy, and therefore, it will takes more time for car to cover the remaining distance in the alternative strategy. This alternative strategy, therefore, can not take less time compared to the optimal strategy. Similarly, we can prove that any modication to the optimal strategy will lead a bigger $T$ value. Therefore, by using the example of $p=0$, we have shown the solution \ref{eq_Theory_T} and \ref{eq_Theory_u} is indeed optimal with respect to different $p$ for all the $p$ in the uncertainty set $ \mathbb{P}=[p_l, p_u] =[0,9]$. The details of such proof can be found in paper \cite{MatSch22}. 


%Our rocket car problem is a typical bang-bang control problem, its optimal solution is obtained when the trajectory takes the extreme points whenever possible. This means acceleration, velocity and deceleration take the maximum value whenever possible. A more theorectical proof of general bang-bang control problem needs background knowledge in calculus of variations, Hamilton–Jacobi–Bellman equation, and the Pontryagin Maximum Principle etc. The paper \cite{KM16} gives a proof and explanation of the solution to the bang bang problem within a bilevel OCP. We refrain from diving deep into these theories, readers can refer to papers \cite{EJ89}, \cite{RV99} and \cite{BD05} for more information. 

The theoretical solution (equations \ref{eq_Theory_T} and \ref{eq_Theory_u}) to our problem \ref{rc} can also be obtained by solving the underlying partial differential equation \ref{rc_partial} directly, while taking the constraints into consideration. The proof are given in Appendix B of \cite{MatSch22}. The proof is tedious in detail, yet the underlying idea is simple and straightforward. Here we will briefly explain the idea behind the proof given in the paper \cite{MatSch22} and show with one example that the theoretical solution in formulation \ref{eq_Theory_T} and \ref{eq_Theory_u} is indeed correct. We take a fixed parameter value $p=0$ as an example for the explanation.

Because the objective is to minimize the time between the starting state and ending state, then it is optimal to run at the maximum allowable velocity whenever possible, i.e with $x_2 =4$ as in the constraint \ref{rc_x2_tc}. Otherwise, time is wasted if a car is running at a velocity that is less than the maximum allowable velocity, i.e. $x_2=4$. Since the starting state is at position $0$ and velocity $0$, to reach the maximum allowable velocity as soon as possible, the car should be accelerated as strongly as possible at the begining until the velocity reaches $x_2 =4$. Then the car should be running at this maximum velocity for a certain period. After this period, the car should be decelerated as strongly as possible until the velocity decreases to $0$, and exactly at the moment that the velocity reaches $0$, the position should reach $10$. It is this moment that the optimal/smallest $T$ is achieved. How long the period should be running at maximum velocity, depends on the acceleration/deceleration distance, which in turn depends on  starting/ending position (in our case, it is $0$ and $10$ respectively) and starting/ending velocity (in our case, both are $0$). 


%where $\mathbf{x}$ is a vector of decision variables, $f(\mathbf{x})$ is the objective function, and $\mathbf{a}$ and $\mathbf{b}$ are vectors of lower and upper bounds on the decision variables, respectively. BOBYQA is a derivative-free optimization algorithm that iteratively builds quadratic models of the objective function and seeks the next point to evaluate based on the predicted improvement of the model. It has been shown to be efficient and effective for optimizing complex, black-box functions subject to box constraints.

%To use BOBYQA for this problem, the objective function $F_i$ needs to be evaluated for each $p_i$ value in $\mathbb{P}$, and the maximum value of $F_i$ needs to be found. For each evaluation of $F_i$, the lower-level optimization problem needs to be solved using BOBYQA algorithm. The decision variables $x(\cdot)$ and $u(\cdot)$ need to be treated as parameters for the upper-level optimization problem, and the derivative-free BOBYQA algorithm can be used to maximize the objective function over $p_i \in \mathbb{P}$.

%The BOBYQA algorithm can solve the lower level OCP of the training approach \ref{maxmin} for a given $p$ because the lower level problem can be re-written into the form of \ref{DFO_bc} and the uncentainty set $\mathbb{P}$ is chosen to be box-shaped. With the BOBYQA algorithm computing local extrema, the upper level problem still needs to be solved globally. This is straightforward in our rocket car case, i.e. maximizing over all $p$.


\begin{equation}
	\begin{aligned}
		\underset{p  \in   \mathbb{P}}{max} \ \underset{x(\cdot), u(\cdot)}{min} &  \ \ F(x(\cdot), u(\cdot), p)\\ 
		s.t.\ \    &  \dot{x} (t) = f(x(t), u(t), p)\\ 
		& x(t) \in \Omega \\
		&  u(t) \in \mathbb{U}  \\
		& p  \in   \mathbb{P}  \\
		& t \in [t_0, t_f]
	\end{aligned}
	\label{maxmin}
\end{equation}

\begin{equation}
	\begin{aligned}
		\underset{p \in \mathbb{P}}{max} \ F(x^(p), u^(p), p)\\
		s.t.\ \ & \dot{x} (t) = f(x(t), u(t), p)\\
		& x(t) \in \Omega \\
		& u(t) \in \mathbb{U} \\
		& t \in [t_0, t_f]\\
		& x^(p) = \underset{x(\cdot)}{argmin}\ \ F(x(\cdot), u^(p), p)\\
		& u^(p) = \underset{u(\cdot)}{argmin}\ \ F(x^(p), u(\cdot), p)\\
	\end{aligned}
	\label{bilevel}
\end{equation}
Here, $x^(p)$ and $u^(p)$ represent the optimal solutions of the lower level problem, obtained by minimizing the objective function $F$ with respect to $x(\cdot)$ and $u(\cdot)$, respectively. The outer maximization problem seeks to find the optimal value of $p \in \mathbb{P}$ that maximizes the minimum objective function value of the inner minimization problem over $x(\cdot)$ and $u(\cdot)$. The state dynamics $\dot{x} (t) = f(x(t), u(t), p)$ and constraints on $x(t)$ and $u(t)$ are the same for both the inner and outer problems. The solution of the outer problem provides the optimal value of $p$ and the corresponding optimal solutions of the lower level problem $x^(p)$ and $u^(p)$.


%the BOBYQA algorithm has limitations, with several strong assumptions being made. Firstly, it has been assumed that the uncertainty set is of moderate size and is box-shaped. Secondly, it has been assumed that there is only one local extremum, i.e., the lower level problem has only one solution for each $p \in \mathbb{P}$. In general, we cannot expect the second assumption to be valid. Although the BOBYQA algorithm is a gradient-free method with respect to the objective function $F(\cdot)$, it still utilizes the gradient of the approximation function $Q(\cdot)$ while updating the iteration. Therefore, this BOBYQA algorithm, or a general DFO approach, is still subject to numerical errors and computational costs while calculating the gradients of the approximation function $Q(\cdot)$  and updating them in each iteration.

%The algorithm of SQP with line search and quasi Newton method for solving NLP with constraints can be summarized as follows 

%\begin{algorithm}
%	\caption{SQP algorithm}\label{alg:cap}
%	\begin{algorithmic}
	%		\Require Conduct a preliminary analyis of the problem and have a reasonable initialization for the variable $x, \lambda, \mu$ as $x_1, \lambda_1, \mu_1$. Evaluate the function $f, \nabla f, $
	%		\Ensure $y = x^n$
	%		\State $y \gets 1$
	%		\State $X \gets x$
	%		\State $N \gets n$
	%		\While{$N \neq 0$}
	%		\If{$N$ is even}
	%		\State $X \gets X \times X$
	%		\State $N \gets \frac{N}{2}$  \Comment{This is a comment}
	%		\ElsIf{$N$ is odd}
	%		\State $y \gets y \times X$
	%		\State $N \gets N - 1$
	%		\EndIf
	%		\EndWhile
	%	\end{algorithmic}
% \end{algorithm}


Multiple shooting combined with Newton-type methods is used for finding the numerical solution of the OCP. In this approach, a sequential programming approach is used to directly solve a subproblem with constraints. 

%The max function ensures that the penalty term is only added if the constraint is violated. If the constraint is not violated, the penalty term is zero and has no effect on the cost function. By adding a penalty term to the cost function for each constraint violation, we can encourage the solver to find solutions that satisfy the constraints. The penality term can be defined in other forms as well. Nevertheless, for the numerical solution of the OCP, we have comabined the mutiple shooting with Newton type methods, within which a squential programming approach is used to solve a subproblem with constriants directly.  


%To solve the OCP as in equation \ref{P2_OPM}, we first discretize the time horizon $ \in [t_0, t_f]$ into multiple subintervals $t_0 = \tau_0 < \tau_1 < ... < \tau_m = t_f$, the time grid will be the same for the control and state variables. Then we choose the support function for the control variables as a constant function, i.e. $ \psi_j(t;w_j)= w_j, t \in \mathbb{I}_j$. We choose a constant control value in each subinterval for two reasons: first, this is easy to implement; and second, we can still get the correct numerical solution fast for our chosen case. Since the control variable support function is chosen to be a constant value for each subinterval, we, therefore, need to introduce the initial guess for $x$ and $u$ in each subinterval, denoted as $x(\tau_j) = s_j$ and $u_j(t) = w_j, t \in \mathbb{I}_j = [\tau_j, \tau_{j+1}]$

%\begin{itemize}
\item Step 1, choose multiple shooting nodes $t_0 = \tau_0 < \tau_1 < ... < \tau_m = t_f$ 
\item Step 2, choose an initial guess $s_j$ and $w_j$ for each subinterval $\mathbb{I}_j$, with $x(\tau_j) = s_j$ and $u_j(t) = w_j$ for $t \in \mathbb{I}_j = [\tau_j, \tau_{j+1}]$, where $j = 0, 1, ..., m-1$. 
\item Step 3, solve the dynamic system using the numerical integration method (see appendix \ref{App1}) and obtain a solution $x(t; s_j, w_j)$ for $t \in \mathbb{I}_j = [\tau_j, \tau_{j+1}]$.
\item Step 4, we need to check whether the solution obtained in Step 3 satisfies the constraints or not. If the solution satisfies the constraints, we can directly compute the cost of the error for the subinterval, denoted as $F_j$. However, if the solution violates any constraint, we need to add a penalty term to the cost function to account for the violation.
$$ F_j = \int_{\tau_j}^{\tau_{j+1}} \mathcal{L}(t, x(t;s_j, w_j), u(t;w_j)) dt + \Phi(x(\tau_{j+1}), \tau_{j+1}) - \Phi(x(\tau_j), \tau_j) $$
%we check whether the solution is feasible (i.e., satisfying the constraints in each subinterval). If the solution is feasible. We compute a cost of the error for the subinterval $\mathbb{I}_j$, which is denoted as $F_j$. We define the cost function as the sum of $F_j$ over all the subintervals. If the solution is not feasible, we add a penalty term to the cost function for each violation of the constraint. 
\item Step 5, compute the cost function value, if such value  is below some tolerance level, stop; otherwise, update the guesses for the state and control variables in each subinterval and go back to Step3. %until the cost function value is below some tolerance level.
%we compute the cost function value, and if this value is below some tolerance level, stop; otherwise, we update the guesses $s_j, w_j, j =0, 1, ..., m-1$ and go to Step 3.
\end{itemize}

% $$ F_j = \left\Vert x(\tau_{j+1};s_j,w_j)-\xi(\tau_{j+1})\right\Vert^2 $$
%$$F_j = L(x(\tau_j), u_j(\tau_j)) \Delta t_j + \frac{1}{2}(x(\tau_{j+1})-s_{j+1})^TQ(x(\tau_{j+1})-s_{j+1})\Delta t_j + \frac{1}{2}(u_{j+1}(\tau_{j+1})-w_{j+1})^TR(u_{j+1}(\tau_{j+1})-w_{j+1})\Delta t_j$$
%$$F_j = \int{\tau_j}^{\tau{j+1}} L(x(t;s_j,w_j), u(t;w_j)) dt + E(x(\tau_{j+1};s_j,w_j))$$
%where $w_j$ is an introduced variable of the control support function for subinterval $ \mathbb{I}_j$. For piecewise constant controls, $u(t)$ takes a constant value $w_j$ in the subinterval $\mathbb{I}_j$. For the piecewise linear controls, $u(t)$ takes the value from the linear interpolation between the left boundary $w_j^l$ and right boundary $w_j^r$ in the subinterval $\mathbb{I}_j$.  The continuity regarding the control variables can be enforced by additional equality 
\begin{equation}
	\psi_j(\tau_{j+1};w_j) =  \psi_{j+1}(\tau_{j+1};w_{j+1})
\end{equation}

To illustrate the concept of the shooting method to solve the boundary value problem (BVP), we use the following example.
\begin{align*}
	\dot{x} = x(t), t_0 \leq t \leq t_f 
\end{align*}
The analytical solution of above equation is 
\begin{align*}
	x(t) = x(t_0)e^{t - t_0}
\end{align*}
where $e$ is the exponential number. For a given value $b$, the initial condition $x(t_0) = x_0$ will be determined to satisfy $x(t_f) = b$. As a result, we get the equation $x(t_f)-b = 0$ or $x(t_0)e^{t_f - t_0}-b = 0$. This derivation is called the shooting method. Generally, the shooting method can be summarized as follows
\begin{description}
	\item[shooting method] \
	\begin{itemize}
		\item Step 1, choose an initial value $x(t_0)=x_0$ 
		\item Step 2, form a solution of the differential equation from $t_0$ to $t_f$
		\item Step 3, define a cost function for the error at the boundary and evaluate such cost function, and if the cost function value is below a tolerance level, then stop, otherwise continue to Step 4 
		\item Step 4, update the guess for $x(t_0)$ based on some updating schema, go to Step 2
	\end{itemize}
\end{description}
In Step 2, with initial value $x(t_0)=x_0$, numerical integration (see appendix \ref{App1}) is used in practice to obtain the solution $x(t), t \in [t_0, t_f]$. The details of the numerical inegration method is given in appendix \ref{App1}. In Step 3, the cost function can be defined as $(x(t_f)-b)^2$, and the tolerance level is set as a reasonable small number, e.g. $1e-8$. The tolenrance level is set based on a balance of accuracy and computation cost (with respect to time). 


In the multiple shooting method, the "shooting" interval is partitioned into short subintervals. We consider a differential equation with boundary value of the following form:
\begin{equation}\label{eqn:ori_dae}
	\begin{aligned}
		& \dot{y} = f(t, y) \
		& y(t_f) = y_f \
	\end{aligned}
\end{equation}
Here, $y$ denotes the differential variables, $t \in [t_0, t_f]$, and $y_f$ is the boundary value at $t_f$. With the multiple shooting method, a suitable grid of multiple shooting nodes $\tau_j \in [t_0,t_f]$, where $t_0 = \tau_0 < \tau_1 < ... < \tau_m = t_f$, is chosen. There are $m$ subintervals covering the whole interval. At the beginning of each subinterval, $[\tau_j, \tau_{j+1}], j = 0, 1, ..., m-1$, we have the initial guess of the starting value $\hat{y}_j$. Then in each subinterval, we have the initial value problem of the following form:

\begin{equation}\label{eqn:msh}
	\begin{aligned}
		& \dot{y} = f(t, y) , t \in [\tau_j, \tau_{j+1}], \ j = 0, 1, ..., m-1  \\
		& y(\tau_j) = \hat{y}_j, \ j = 0, 1, ..., m-1 \
	\end{aligned}
\end{equation}

In each subinterval, we introduce the new unknown parameter $\hat{y}_j$, solve an initial value problem, and obtain a solution $y(t), t \in [\tau_j, \tau_{j+1}]$. The piecewise solution is not necessarily continuous and does not necessarily satisfy the boundary condition $y(t_f) = y_f$. The continuity has to be enforced by additional matching conditions at each subinterval boundary, i.e.,

\begin{equation}\label{eqn:mc}
	\begin{aligned}
		& y(\tau_{j+1}; \hat{y}j) = \hat{y}_{j+1}, \ j = 0, 1, ..., m-1  \\
		& \hat{y}_{m} (i.e. \ \hat{y}_{\tau_m} = \hat{y}_{t_f}) =  y_f 
	\end{aligned}
\end{equation}



%The multiple shooting method is well suited for solving optimal control problems of the form as in the equation \ref{P2_OPM}, as it can serve as the "first discretize" part of the direct approaches. For OCPs in the form of the equation \ref{P2_OPM}, we follow the same idea of how the multiple shooting method can be used to solve boundary value problems. For OCPs, there are both control variables and state variables; therefore, we need to discretize both the control and state variables. For simplicity, we choose the same discretization grid points for the control variables and the state variables. For the control variables in each subinterval, we introduce a 'local support' ansatz function. 

%Therefore, the first step remains the same, i.e., choosing the suitable time grid to discretize the whole interval $[t_0, t_f]$ into $m$ subintervals $t_0 = \tau_0 < \tau_1 < ... < \tau_m = t_f$, with the $i$-th subinterval as $\mathbb{I}_j = [\tau_j, \tau_{j+1}]$. In the second step, we define a local support function for the control variables. The local support function for the control variables in subinteval $\mathbb{I}_j$, which is denoted as  $u(t) \mid_{t \in \mathbb{I}_j} = \psi_j(t;w_j), t \in \mathbb{I}_j$  can be of the following type


%The problem \ref{eqn:tdode} is similar to the problem \ref{eqn:ori_dae}. In each subinterval, a boundary value problem (BVP) is to be solved, and matching conditions at the boundary of each subinterval are to be enforced so that the final solution is continuous and applicable to the whole interval.

%In the examples above, no control variables exist, so only an initial guess $\hat{y}_k$ needs to be introduced for each subinterval. For optimal control problems, piecewise local support functions need to be introduced in the mutilphe shooting method to address the control variables, and initial guesses for the state variables need to be provided separately.

In multiple shooting method, the "shooting" interval is partitioned into some short subintervals. We consider a differential equation with boundary value of the following form
\begin{equation}\label{eqn:ori_dae}
	\begin{aligned}
		& \dot{y} = f(t, y) \\ 
		& y(t_f) = y_f  \\
	\end{aligned}
\end{equation}
Where $y$ denotes the differential variables, $t \in [t_0, t_f]$, $y_f$ is the boundary value at $t_f$. With multiple shooting method, one chooses a suitable grid of multiple shooting nodes $\tau_j \in [t_0,t_f] $, where $t_0 = \tau_0 < \tau_1 < ... < \tau_m = t_f$, i.e. $m$ subintervals covering the whole interval. At the beginning of each subinterval, $[\tau_k, \tau_{k+1}], k = 0, 1, ..., m-1$, we have the initial guess of the starting value $\hat{y}_k$. Then in each subinterval, we have the initial value problem of the following form: 
\begin{equation}\label{eqn:msh}
	\begin{aligned}
		& \dot{y} = f(t, y) , t \in [\tau_k, \tau_{k+1}], \ k = 0, 1, ..., m-1   \\ 
		& y(\tau_k) = \hat{y}_k, k = 0, 1, ..., m-1  \\
	\end{aligned}
\end{equation}
In each subinterval, we introduce the new unknown parameter $\hat{y}_k$, we solve an initial value problem and will get a solution $y(t), t \in [\tau_k, \tau_{k+1}]$. The piecewise solution is not necessary continuous and also not necessarily satisfy the boundary condition $y(t_f) = y_f$. The continuity has to be enforced by additional matching conditions at each subinterval boundary, i.e. 
\begin{equation}\label{eqn:mc}
	\begin{aligned}
		& y(\tau_{k+1}; \hat{y}_k) = \hat{y}_{k+1}, \  k = 0, 1, ..., m-1  \\
		& \hat{y}_{m} (i.e. \ \hat{y}_{\tau_m} = \hat{y}_{t_f}) =  y_f 
	\end{aligned}
\end{equation}


%Nevertheless, the BOBYQA algorithm has limitations with several strong assumptions being made. Firstly, it has been assumed the uncertainty set is of moderate size and is box-shaped. Secondly, it has been assumed that there is only one local extrema, i.e. the lower level problem has only one solution for each $p \in \mathbb{P}$. In general, we cannot expect the second assumption to be valid. The BOBYQA algorithm is a gradient-free method with repect to the objective function $F(\cdot)$, it still utilizes the gradient of the approximation function $Q(\cdot)$ while updating the iteration. Therefore, this BOBYQA algorithm, or a general DFO approach, is still subject to the numerical errors and computational costs while calculating the gradients of the approximation function $Q(\cdot)$  and updating them in each iteration.  


% The lower level OCP of the Training Approach \ref{maxmin} can be solved with the BOBYQA algorithm for a given $p$, since the lower level problem can be re-written into the form of \ref{DFO_bc} and the uncentainty set $\mathbb{P}$ is chosen to be box-shaped. With the BOBYQA algorithm computing local extrema, the upper level problem still needs to be solved globally. In our rocket car case in Chapter \ref{Chapter4}, this is straight-forward, i.e. maximizing over all $p$. 

%		\underset{x(\cdot), u(\cdot)}{min} \   \underset{p  \in   \mathbb{P} }{max} & \ \ F(x(\cdot), u(\cdot), p)\\ 
%s.t.\ \   &  \dot{x} (t) = f(x(t), u(t), p)\\ 
%& x(t) \in \Omega \\
%& u(t) \in \mathbb{U}  \\
%& p  \in   \mathbb{P}  \\
%& t \in [t_0, t_f]

%In the classical approach, the set of feasible controllable variables are given by $u(\cdot)$, which yield feasible trajectories $x(\cdot, p)$ for all $p  \in   \mathbb{P}$. The value of the objective function in the lower level does not depend on $p$ and $x(\cdot, p)$. In other words, in this approach, the dynamic system has no prior knowledge about the value of the parameter $p$ and gets no feedback during the process and has to decide the control strategy in advance.  In some OCP under uncertainty, it is not possible to find a unified control $u(\cdot)$ which have fesiable solutions for all $p  \in   \mathbb{P}$. In this case, we may need to find the pair for $x(\cdot;p), u(\cdot;p)$ for different $p$, and among the pairs, we find the worst solution.



%			s.t.\ \ &  \dot{x} (t) = f(x(t), u(t)), \ \ (system \ dynamics)   \label{P2_sd} \\
%& g(x(t)) = 0 \  or \leq 0, \ t \in [t_0, t_f]\  (path\  equality\ or\ inequality\ constraints)  \label{P2_ec}\\
%& h(x(t), u(t)) =0\  or  \leq 0,\ t \in [t_0, t_f] \ (mixed \ control-state  \ constraints)  \label{P2_inc}\\
%& x(t_0) = x_0, \ \ (initial \ value) \\
%& r(x(t_f)) \leq 0, \ \ (terminal \ constraints)  \label{P2_final} \\
%& u^{lower} \leq u(t) \leq u^{upper}   \label{P2_box_u} \\ 
%& t \in [t_0, t_f]

%In a simplified notation, the problem can be written as 


%Due to the $min \ max$ notation, this classical approach of solving the bilevel problem can also be called $min max$ approach. 

Based on our discussion regarding Newton-style method, to find the minimum of $	\mathcal{L}(w,\lambda, \mu) $, we are motivated to solve a linear system as in equation \ref{HessianEq}, where the first order derivatives and Hessian matrix is calculated for the objective function $\mathcal{L}(w,\lambda, \mu)$. Starting with an initial guess of $(w_0, \lambda_0, \mu_0)$, then we are motivated to update  $(w,\lambda, \mu)$ so as to decrease the objective function $\mathcal{L}(\cdot)$ by solving a subproblem of quadrative programming of the following form


\begin{subequations}
	\label{eq:ocp_QP}
	\begin{align}
		\underset{\Delta w }{\text{min}} \qquad & \frac{1}{2} \Delta w^\top \nabla^2_w \mathcal{L}(\cdot)\Delta w +\nabla_w f(w_i)^T \Delta w	\\
		\qquad \text{s.t.}\qquad	& g(w_i) + \nabla g(w_i)^T \Delta w = 0 \\
		& h(w_i) + \nabla h(w_i)^T \Delta w \leq 0
	\end{align}
	\label{P4_QP}
\end{subequations}
where $\nabla^2_w \mathcal{L}(\cdot)$ is the Hessian matrix of function $\mathcal{L}(\cdot)$, whose updating can be calculated with quasi Newton method. With a solution $\Delta w_k, \lambda_k^{QP}, \mu_k^{QP}$ found from the subproblem \ref{P4_QP}, we can therefore update the $w, \lambda, \mu$ as 
\begin{subequations}
	\begin{align}
		w_{k+1} &= w_k + \Delta w_k \\
		\lambda_{k+1} &= \lambda_k^{QP} \\ 
		\mu_{k+1}&= \mu_k^{QP} 
	\end{align}
\end{subequations}
until convergence. 

%This construction is done in such a way that the sequence $x_k, k =1, 2, 3, ...$ converges to a local minimum $x^\star$ of the NLP \ref{eq:OCP_discret_compact} as $k \approx \infty$.

%$w = (x_0, u_0, x_1, u_1, ..., x_{m-1}, u_{m-1}, u_m)$ piecewise, and they are  in the form of equality constraints or inequality constraints. 


%the initial guess of $x$ and $u$ for each subinterval $\mathbb{I}_j = [\tau_{j-1}, \tau_j]$ and the final control value $u_m$, i.e. $w = (x_0, u_0, x_1, u_1, ..., x_{m-1}, u_{m-1}, u_m)$. The constraints in formulation \ref{P3_MSNLP} are applied to the independent variables $w = (x_0, u_0, x_1, u_1, ..., x_{m-1}, u_{m-1}, u_m)$ piecewise, and they are  in the form of equality constraints or inequality constraints. Together with the matching condition \ref{P3_eq}, the NPL problem  \ref{P3_MSNLP} can be re-written as the form in NLP \ref{eq:OCP_discret_compact}, with the $x$ in NLP \ref{eq:OCP_discret_compact} replaced by $w = (x_0, u_0, x_1, u_1, ..., x_{m-1}, u_{m-1}, u_m)$. -

% , i.e. including the constraints and the original objective function into a new objective function, defined as a

   %i.e. we have $x_0, x_1, ..., x_{m-1}$ and $u_0, u_1, ..., u_m$. Then in each subinterval $\mathbb{I}_j = [\tau_j, \tau_{j+1}]$, the following step should be taken: 
%, with the equality constraints $g(x)=0$ coming from the matching codition \ref{P3_eq} and the original equality constraints \ref{P2_ec} of OCP \ref{P2_OPM}. The other constraints of  OCP \ref{P2_OPM}, as in \ref{P2_inc}, \ref{P2_final}, \ref{P2_box_x}, and \ref{P2_box_u} can be written into the $h(x) \leq 0$ part of  NLP \ref{eq:OCP_discret_compact}. 
%\ref{P3_constraint} refers to equation  \ref{P2_ec}, \ref{P2_inc}, \ref{P2_final}, \ref{P2_box_x}, and \ref{P2_box_u}, which are applied 


% Since this NLP transformed from the original OCP  \ref{P2_OPM}, we are solving the original OCP within an iterative sequential quadratic programming (SQP) approach.

% with  transferred OCP (), i.e. the problem  \ref{P3_MSNLP} with the form \ref{P2_OPM} in details, i.e. working 


%Solving the problem \ref{BFGSBUp} directly is not trivial, but we can prove that problem ends up being equivalent to updating our approximate Hessian $B$ at each iteration by adding two symmetric, rank-one matrices $U$ and $V$:

%\begin{align*}
%\begin{multline*}
% in this section, as we can apply KKT condition to incorporate constraints into a new objective function.
% How the quasi Newton method, together with multiple shooting, can be used to solve optimal control problem, will be exlained in more details in Chapter \ref{Chapter4} when we discuss our numerical solutions for the rocket car case. 

%can be expanded for problems with constraints, will be explained in the more details in Section \ref{Sec_KKT}. 

%transform the constrained problem into an unconstrained form, we can, therefore, directly apply algorithms that are suitable for unconstrained minimization. 

% then we can find a local mininizer for the NLP. If such NLP is a convex problem, then the local minimizer will be the global mininizer.  
%KKT condition can be applied to include the constraints into a Lagrange function, and we can employ an updating schema with an initial guess to solve NLP as in equation \ref{eq:OCP_discret_compact} and optimal control problem as in equation \ref{P2_OPM}. 

%we can, reformulate the NLP as in equation \ref{eq:OCP_discret_compact} as well as the optimal control problem as in equation \ref{P2_OPM}, and then re-write in the Lagrange form \ref{eq_Lagrangian}, optimize the (objective) Lagrange function with quasi Newton method. 
%The Karush–Kuhn–Tucker (KKT) approach to NLP can include both equality constraints and inequality constraints, together with the original objective function,
%The gradient descent method starts with an initial guess and follows a direction opposite the gradient, which decreases the Lagrange function to reach the optimal point. Therefore, gradient descent method is a first-order iterative optimization algorithm, but often suffers slow convergence. Instead,

%  In another word, for any $p$, its corresponding solution $min \  F(\cdot; p)$  should smaller or equal to the conservative solution. 

%For each index $i$, they define an $L-$shaped set in the $(h_i, \mu_i)$ space. This set is not a smooth manifold but has a non-differentiability at the origin, i.e.  

%\begin{definition}(Feasible set) The feasible set $\Omega$ is the set 
%	\begin{align}
	%		\Omega:= \{x \in \mathbb{R}^n \ | \ g(x)= 0 , \ h(x)\leq 0 \}
	%	\end{align}
%\end{definition}

%Since the approaches we are going to use in this paper will be demonstrated with the case of rocket car, we decide to describe the rocket car case first. So that, when we are discussing our approaches, we can directly describe how they can be used in solving the rocket car case. 

%The main focus (of solving the Cerebral Palsy problem) of the paper \cite{MatSch22}, is the "worst-case treatment planning by bilevel optimal control", i.e. a bilevel optimization problem. The bilevel optimisation method in paper \cite{MatSch22} solves the parameterized optimization problems, e.g. the Cerebral Palsy problem, in a conservative way.

%In the text that follows, we discuss our method with in the form as equation  \ref{P2_OPM}
%For real-life problems, practical problems, it could means using the least energy travelling from initial point to terminal point.  
%$x(t) \in \Omega$ represents the constraints for which the state variables $x(t)$ must satisfy, usually it is in the form of a set of differential equations describing the path of state variables, and/or with some equality and inequality constraints. $ u(t) \in \mathbb{U}$ represents the constraints for which the control variables $u(t)$ must satisfy, and $\mathbb{U}$ is usually a convex set. In many cases, the constraints on $x(t)$ and $u(t)$ comes together, i.e. functions of $x(t)$ and $u(t)$ must satisfy some conditions. The choice of the control variable $u(t)$ will have an effect on the value of the state variable $x(t)$, therefore, will affect the objective function value $\Psi(\cdot)$. 


%The actual form of the mathematical represenation may vary,
%The actual form of the mathematical represenation may vary,
%   \begin{subequations}
	%	\begin{align}
		%		\underset{T, u(\cdot)}{min} \  \underset{ p \in \Omega_P, x(\cdot,p)}{max}  \ \   & \  T \\ 
		%		s.t.  & \ \ x = (x_1, x_2)   \label{ca_rc_x} \\ 
		%		& \ \  \dot{x} = T  \begin{pmatrix}  x_2(t;p) \\ u(t)-p   \end{pmatrix}, & \ t \in [0,1],  \label{ca_rc_partial} \\
		%		& \ \ x(0,p) = 0, \label{ca_rc_t0}\\
		%		& \ \ x_1(1;p) \geq 10, & \ for \ all \ p \in \Omega_P, \label{ca_rc_x1_t1} \\
		%		& \ \ x_2(t;p) \leq 4, & t \in [0,1], \ for \ all \ p \in \Omega_P,  \label{ca_rc_x2_tc} \\
		%		& \ \ x_2(1;p) \leq 0, & \ for \ all \ p \in \Omega_P, \label{ca_rc_x2_t1}  \\
		%		& \ \ T \geq 0, \\
		%		& \ \ u(t) \in [-10, 10], & t \in [0,1]. 
		%	\end{align}
	%	\label{ca_rc}
	%\end{subequations}
	
	% Without 
	%  When solving optimal control 
	% Usually, for the real life problems, the optimal control problem can be  expressed in a  mathematical form of the following:  problem expressed
	%it may be in the form of $\{g(x) = 0,  h(x)  \leq  0 \}$ as in the problem \ref{GeneralMin}, i.e. the feasible set $\Omega = \underset{x}{arg} \ \{ g(x) = 0,  h(x)  \leq  0 \}$. Various methods exist for handling the constraints, such as  Karush–Kuhn–Tucker (KKT) method discussed in Section \ref{Sec_KKT}. After transforming the constrained problem into an unconstrained form, we can, therefore, directly apply algorithms that are suitable for unconstrained minimization. For the sake of simplicity, we focus on explaining the Newton and quasi-Newton method without constraints in this chapter. How the quasi-Newton can be expanded for problems with constraints, will be explained in the more details in Section \ref{Sec_KKT}. 
	%A control problem inclues a cost function, which is 
	% The objective function is also called cost function, which is  a fucntion of state and control varibales. 
	
	parameterized optimization problems, such as the therapy design of Cerebral Palsy (CP) problem described in \cite{MatSch22}. In this paper, we focus on using gradient method to solve parameterized optimization problems, with a case study in state constrained rocket car. 
	
	Without giving a rigorous condition and definition\footnote{We do not give a rigorous definition on purpose so that the problem we have described here can be applied to more general cases when such condition and definition are more clearly defined.},  a general optimization problem is typically of the form
	\begin{equation}
		\begin{aligned}
			\underset{x \in \mathbb{R}^n}{\text{min}}  & 	\   f(x) \\
			s.t.  \  \  \ & g(x) = 0, \\ 
			&  h(x)  \leq  0 
		\end{aligned}
		\label{GeneralMin}
	\end{equation}
	where $f(x)$ is the objective or cost function, $g(x) = 0$ and $h(x)  \leq  0$ are the constraints. Some optimization problems may have uncertain parameters whose value are priori unknown, and the optimal objective value depends on the parameter value. This kind of problem is called the parameterized optimization problems and is of the form 
	\begin{equation}
		\begin{aligned}
			\underset{x \in \mathbb{R}^n}{\text{min}}  & 	\    f(x, p) \\
			s.t.  \  \  \ & g(x, p) = 0, \\ 
			&  h(x,p)  \leq  0  \\ 
			& x = x(p) \\
			& x = x(p^0) \  if \  p = p^0 \\
			& p \in \Omega_P		
		\end{aligned}
		\label{ParaMin}
	\end{equation}
	where $p^0$ is a fixed value in the feasible uncertainty set $\Omega_P$, where the parameter $p$ can take value from.
	
	Parameterized optimization problems are very difficult to solve due to the uncertainty in the parameter $p$. In the paper \cite{MatSch22}, multiple methods of solving the parameterized optimization problem have been discussed. The main focus (of solving the Cerebral Palsy problem) of the paper \cite{MatSch22}, is the "worst-case treatment planning by bilevel optimal control", i.e. a bilevel optimization problem. The bilevel optimisation method in paper \cite{MatSch22} solves the parameterized optimization problems, e.g. the Cerebral Palsy problem, in a conservative way.
	
	
	One method of solving the original CP problem in a conservative way is to transform the problem \ref{ParaMin} into another form. Assuming that the parameter $\tilde{p}$ lies in an uncertainty set $\Omega_P$, we can firstly reach one objective, i.e. identifying a worst possible solution with respect to $\tilde{p}$, i.e. solving a lower level problem. Based on the result of lower level, we can continue to find the best solution with respect to $x$, i.e. solving a upper level problem. The "worst-case treatment planning by bilevel optimal control", i.e. a  bilevel optimization problem, is an optimization problem in which another optimization problem enters the constraints. Mathematically, the problem \ref{ParaMin} is transformed into another form, and can be formulated in a simplified notation, as following
	
	%In a simplified notation, the problem can be written as 
	\begin{equation}
		\begin{aligned}
			\underset{x}{min} \   \underset{p \in \Omega_P}{max} & \  f(x,p) \\ 
			s.t.  & \  g(x, p) = 0, \  h(x,p)  \leq  0 \\
		\end{aligned}
		\label{minmax}
	\end{equation}
	
	
	Due to the $min \ max$ notation, this classical approach of solving the bilevel problem can also be called $min max$ approach. 
	
	The paper \cite{MatSch22} use a derivative free method in the Training Approach. This paper at hand will focus on a gradient method to solve the $maxmin$ problem.  In particular, we are interested in how to compute the derivatives theoretically and numerically.  We would like to apply the quasi-Newton and multiple shooting method when solving the problem numerically. The approaches discussed in this paper at hand will be demonstrated with a case study in state constrained rocket car. 
	
	We choose this rocket car case for two reasons: firstly, the case is relatively easy to understand and is quite representative of the general usage in real life; secondly, the case has theoretical solution and we can compare the numerical results with the theoretical value so that we can check whether our gradient method can find the optimal solution and how fast it converges. 
	
	The structure of this paper is as follows: in Chapter 1, i.e. this chapter,  we give an introduction on what problems this paper intends to address. In the Chapter 2, we introduce the case of the state constrained rocket car. In the Chapter 3, we discuss the classical approach and training approach, and show the theoretical value of the chosen case. In Chapter 4, we give the mathematical background of the quasi-Newton and multiple shooting method. In Chapter 5, we show how we can solve the case numerically using the methods described in Chapter 4. In Chapter 6, we compare our theoretical and numerical results and conclude the paper. 
	
	
	
	As stated in \cite{MatSch22}, many different methods can be used to solve a bilevel problem, three approaches have been discussed in detail, i.e. a transformation of the bilevel problem to a single level problem, a classical approach and a training approach. A intuitive approach is to transfer the bilevel problem into a single level problem, however, in general the resulting single level problem is not equivalent to the original bilevel problem and this approach is also out of the focus of the paper \cite{MatSch22} as well as this paper at hand. A classical approach, aka a robust optimization appraoch, is consistent with the $minmax$ appoach, which will be discussed in more detail in Chapter 2.
	
	The paper \cite{MatSch22} introduces the "Training Approach".  It is based on the idea that in the real world, during the training period, an intervention is introduced and a certain, but a priori unknown, parameter $p \in \Omega_P$ is realized. What follows the training period (during which the parameter $p$ is realized), the patient is able to react to it in an optimal manner, i.e. an optimal value $f(x,p)$ will be obtained given the  realized parameter $p$. The paper \cite{MatSch22} call this approach "worst case modeling Training Approach", and it can be written as 
	
	\begin{equation}
		\begin{aligned}
			\underset{p \in \Omega_P}{max} \ \underset{x}{min} & \  f(x,p) \\ 
			s.t.  & \  g(x, p) = 0, \  h(x,p)  \leq  0 \\
		\end{aligned}
		\label{maxmin}
	\end{equation}
	
	Due to the $max \ mix$ notation, this approach of solving the bilevel problem can also be called $max min$ approach. 
	
	The paper \cite{MatSch22} use a derivative free method in the Training Approach. This paper at hand will focus on a gradient method to solve the $maxmin$ problem.  In particular, we are interested in how to compute the derivatives theoretically and numerically.  We would like to apply the quasi-Newton and multiple shooting method when solving the problem numerically. The approaches discussed in this paper at hand will be demonstrated with a case study in state constrained rocket car. 
	
	We choose this rocket car case for two reasons: firstly, the case is relatively easy to understand and is quite representative of the general usage in real life; secondly, the case has theoretical solution and we can compare the numerical results with the theoretical value so that we can check whether our gradient method can find the optimal solution and how fast it converges. 
	
	The structure of this paper is as follows: in Chapter 1, i.e. this chapter,  we give an introduction on what problems this paper intends to address. In the Chapter 2, we introduce the case of the state constrained rocket car. In the Chapter 3, we discuss the classical approach and training approach, and show the theoretical value of the chosen case. In Chapter 4, we give the mathematical background of the quasi-Newton and multiple shooting method. In Chapter 5, we show how we can solve the case numerically using the methods described in Chapter 4. In Chapter 6, we compare our theoretical and numerical results and conclude the paper. 
	
	\label{Chapter1}
	
	
	
	
	\chapter{Rocket car case}
	Since the approaches we are going to use in this paper will be demonstrated with the case of rocket car, we decide to describe the rocket car case first. So that, when we are discussing our approaches, we can directly describe how they can be used in solving the rocket car case. The description of the rocket car case is mostly coming from the paper \cite{MatSch22}, with content either verbatim or in a modified form. 
	
	We consider the rocket car case with state constraints, i.e. the one-dimensional movement of a mass point under the influence of some constant acceleration/deceleration, e.g. modeling head-wind or sliding friction, which can accelerate and decelerate in order to reach a desired position. The mass of the car is normalized to 1 unit\footnote{We do not specify the unit on purpose since the actual unit, either one kilogram or meter, does not play a role in the modeling. We are more concerned about the scale.} and the constant acceleration/deceleration enters the model in form of an unknown parameter $p \in \Omega_P \subset \mathbb{R}$ suffering from uncertainty, with the uncertainty set $\Omega_P$ convex and compact. We consider a problem in which the rocket car shall reach a final feasible position and velocity in a minimum time: 
	
	
	
	\begin{subequations}
		\begin{align}
			\underset{T, u(\cdot), x(:,p)}{min} \   & \  T \\ 
			s.t.  & \ \ x = (x_1, x_2)   \label{rc_x} \\ 
			& \ \  \dot{x} = T  \begin{pmatrix}  x_2(t;p) \\ u(t)-p   \end{pmatrix}, & \ t \in [0,1],  \label{rc_partial} \\
			& \ \ x(0,p) = 0, \label{rc_t0}\\
			& \ \ x_1(1;p) \geq 10, \label{rc_x1_t1} \\
			& \ \ x_2(t;p) \leq 4, & t \in [0,1], \label{rc_x2_tc} \\
			& \ \ x_2(1;p) \leq 0, \label{rc_x2_t1}  \\
			& \ \ T \geq 0, \\
			& \ \ u(t) \in [-10, 10], & t \in [0,1]. 
		\end{align}
		\label{rc}
	\end{subequations}
	
	where $x$ represents the variables of the rocket car, and it has two components $ x = (x_1, x_2)$. The first component $x_1$ is the (time-transformed) position of the rocket car. The second component $x_2$ is (time-transformed) velocity of the rocket car. The condition \ref{rc_t0}, i.e. $x(0,p) = 0$, indicates that at $t=0$, both the position and velocity of the car is $0$. The condition \ref{rc_x1_t1}, i.e. $x_1(1;p) \geq 10$, indicates that the position of the car at $t=1$ must be greater or equal to $10$. The condition \ref{rc_x2_t1}, i.e. $x_2(t;p) \leq 4$, indicates that the velocity of the car is always smaller or equal to 4 across the whole period. The condition \ref{rc_x2_t1}, i.e. $x_2(1;p) \leq 0$, indicates that the velocity of the car at $t=1$ is always smaller or equal to $0$. Here, a negative velocity means that the car is moving in a direction that decreases the position. To make the rocket car case even simpler, we can limit the size of the uncertainty set, as following
	\begin{equation}
		p \in \Omega_P = [p_l, p_u] \subset [0,9],
	\end{equation}
	where $p_l < p_u$, with $p_l$ and $p_u$ the lower and upper boundary of the parameter $p$.  
	
	The decision variable in the problem \ref{rc} is the controllable parameter T, which encodes the process duration of the corresponding problem with free end time. The control function $ u: [0,1] \rightarrow \mathbb{R}$ represents the acceleration/deceleration value, and is dependent on the unknown parameter $p$, as shown in the condition \ref{rc_partial}. The second component of the condition \ref{rc_partial}, i.e. $\dot{x_2} = T (u(t)-p)$, indicates the change in the velocity of the car at time $t$ is subject to the value of $T, u(t)$ and $p$. The first component of the condition \ref{rc_partial}, i.e. $\dot{x_1} = Tx_2(t;p)$, indicates the position of the car at time $t$ is subject to the value of $T$ and the velocity $x_2(t;p)$ at time $t$. The variable $x(t:p)$ is a dependent variable, and is uniquely determined by $T, u(\cdot)$ and $p$. The goal is to minimize $T$ such that the variable $x(t:p)$ satisfies all the conditions in \ref{rc}. 
	
	\section{Theoretical solution to rocket car case}
	As explained in Chapter \ref{Chapter1}, we choose the rocket car case for two main reasons, i.e. the easyness of understanding and the existence of theoretical solution, which will be shown in this section. 
	
	
	
	%There are three optimization variables in the optimization problem \ref{rc}, i.e. $T, u$ and $x$, and they belong to the following normed space
	%\begin{equation}
	%	(T, u(\cdot), x(:,p)) \in \mathbb{R} \times \mathbb{L}^\infty([0,1], \mathbb{R}) \times  \mathbb{W}^{1,\infty}([0,1], \mathbb{R}^2)
	%\end{equation}
	The optimization problem \ref{rc} has a unique global solution, and no further local solution exists. The optimal controllable parameter is given by 
	\begin{equation}
		T^\star = T^\star(p) = 2.5 + \frac{40}{100-p^2},
	\end{equation}
	and the optimal control function $u^\star(\cdot) (= u^\star(\cdot; p))$ by 
	\begin{equation}
		u^\star(\cdot) =     \left\{
		\begin{array}{ll}
			10, & for \  0 \leq t <  \frac{4}{(10-p)T^\star}\\
			p  &  for \ \frac{4}{(10-p)T^\star} \leq t < 1- \frac{4}{(10+p)T^\star} \\
			-10  & for \  1- \frac{4}{(10+p)T^\star} \leq t \leq 1 
		\end{array}
		\right.
	\end{equation}
	
	In words, we accelerate as strongly as possible (the acceleration value $u^\star(t)=10$) until the velocity $x^\star_2(t;p)=4$, and then keep the velocity $x^\star_2(t;p)$ constant for a certain periofd of time\footnote{The acceleration value cancels out with a inherent deceleration value so that the velocity can stay constant. The inherent deceleration value can be result of a friction or head wind.}, and eventually decelerate as as strongly as possible until the velocity is $x_2(1;p) \leq 0, \label{rc_x2_t1}$. The moment $x_2(1;p)$ reaching the value of $0$ is the moment that we finds the optimal/smallest $T$ that all the conditions are satisfied. 
	
	The proof of the theoretical solution is given in Appendix B of \cite{MatSch22}. Because of the simplicity nature of the rocket car case, we can find the theoretical solution of the nominal/original problem for our case \ref{rc}. But for many real life problems, it is very difficult to find a direct solution to the original problem, and for some cases not feasible, due to the uncertainty in the parameter $p$. That is why in the paper \cite{MatSch22}, a classical (in the form $minmax$) approach and a training (in the form $maxmin$) approach have been discussed, and both approaches will lead to a conservative solution to the original problem. A conservative solution to the CP problem is a acceptable (or desired) result since less risk should be taken regarding the therapy design of CP problem. In the next chapter, we discuss, in details, the classical approaches and training approach for the rocket car problem. After that, we focus on the quasi-Newton and multi shooting approach to the same problem.  
	
	
	
	
	
	\chapter{The Classical and Training Approach}
	
	The paper on hand focuses on using quasi-Newton and multi-shooting method for the Training Approach. In this chapter, we shortly introduce the Classical Approach first and then we discuss the Training Approach in greater detail. In the next chapter, we can introduce the quasi-Newton and multi-shooting method, and elaborate in detail how they can be used for the Training Approach. 
	
	
	
	\section{The Classical Approach}
	
	As stated in the introduction part, the classical approach is consistent with the $minmax$ approach, during which, two level optimization problems are solved. 
	
	In the lower level, we solve an optimization problem ($max \  f(x,p)$) with respect to $p$, and in the upper level, we continue to find the best solution with respect to $x$, as shown in \ref{minmax}. In the case of the rocket car, the classical approach will be expressed in the following form
	
	\begin{subequations}
		\begin{align}
			\underset{T, u(\cdot)}{min} \  \underset{ p \in \Omega_P, x(\cdot,p)}{max}  \ \   & \  T \\ 
			s.t.  & \ \ x = (x_1, x_2)   \label{ca_rc_x} \\ 
			& \ \  \dot{x} = T  \begin{pmatrix}  x_2(t;p) \\ u(t)-p   \end{pmatrix}, & \ t \in [0,1],  \label{ca_rc_partial} \\
			& \ \ x(0,p) = 0, \label{ca_rc_t0}\\
			& \ \ x_1(1;p) \geq 10, & \ for \ all \ p \in \Omega_P, \label{ca_rc_x1_t1} \\
			& \ \ x_2(t;p) \leq 4, & t \in [0,1], \ for \ all \ p \in \Omega_P,  \label{ca_rc_x2_tc} \\
			& \ \ x_2(1;p) \leq 0, & \ for \ all \ p \in \Omega_P, \label{ca_rc_x2_t1}  \\
			& \ \ T \geq 0, \\
			& \ \ u(t) \in [-10, 10], & t \in [0,1]. 
		\end{align}
		\label{ca_rc}
	\end{subequations}
	
	In the Classical Approach, the set of feasible controllable parameters and control functions are given by those $T$ and $u(\cdot)$, which yield feasible trajectories $x(\cdot, p)$ for all $p \in \Omega_P$. The value of the objective function in the lower level does not depend on $p$ and $x(\cdot, p)$. In other words, in this approach, the driver has no prior knowledge about the value of the parameter $p$ and gets no feedback during the process and has to set up the driving strategy in advance. 
	
	\section{The Training Approach}
	Contrast to the Classical Approach, in the Training Approach it is assumed that the driver of the rocket car is able to perform optmially for every $p$ because of a preceding training period. Thus the worst possible optimal performance is given by a solution of the problem
	\begin{subequations}
		\begin{align}
			\underset{p \in \Omega_P, T, u(\cdot), x(\cdot,p)}{max}  \ 	\underset{}{min} \   & \  T \\ 
			s.t.  & \ \ x = (x_1, x_2)   \label{ta_rc_x} \\ 
			& \ \  \dot{x} = T  \begin{pmatrix}  x_2(t;p) \\ u(t)-p   \end{pmatrix}, & \ t \in [0,1],  \label{ta_rc_partial} \\
			& \ \ x(0,p) = 0, \label{ta_rc_t0}\\
			& \ \ x_1(1;p) \geq 10, \label{ta_rc_x1_t1} \\
			& \ \ x_2(t;p) \leq 4, & t \in [0,1], \label{ta_rc_x2_tc} \\
			& \ \ x_2(1;p) \leq 0, \label{ta_rc_x2_t1}  \\
			& \ \ T \geq 0, \\
			& \ \ u(t) \in [-10, 10], & t \in [0,1]. 
		\end{align}
		\label{TA_rc}
	\end{subequations}
	
	The solution of the Training Approach in paper \cite{MatSch22} is given by a gradient-free method, more precisely, a so-called model-based Derivative-Free Optimization (DFO) approach for box-constrained optimization problems is used. The BOBYQA algorithm is chosen for such approach to solve problems of the form
	\begin{equation}
		\begin{aligned}
			\underset{x \in \mathcal{R}^n}{min} & \  F(x)  \\ 
			s.t.  & \ a_i \leq x_i \leq b_i, i = 1, ..., n \\
		\end{aligned}
		\label{DFO_bc}
	\end{equation}
	
	The name BOBYQA is an acronym for "Bound Optimization BY Quadratic Approximation", and is used to solve lower level problem of \ref{TA_rc}. In the general DFO method, the objective function $F(\cdot)$ is considered a black box. For a given $p$, the parametric lower level OCP of the Training Approach \ref{TA_rc} is solved with a direct DFO approach and the resulting (finite dimensional) solution is viewed as dependent variable. Furthermore, the uncentainty set $\Omega_P$ is box-shaped, and hence the BOBYQA algorithm is applicable to the problem in the Training Approach. The BOBYQA algorithm has been introduced in details in the paper \cite{MicPow09}, and we reiterate the main idea in the text that follows.  
	
	The method of BOBYQA is iterative, $k$ and $n$ being reserved for the iteration number and the number of variables, respectively. Further, $m$ is reserved for the number of interpolation conditions that are imposed on a quadratic approximation $Q_k(x) \xrightarrow{} F(x), \ x \in  \mathcal{R}^n$, with $m$ is a chosen constant  integer from the interval $[n+2, \frac{1}{2}(n+1)(n+2)]$. 
	
	The approximation is available at the beginning of the $k$-th iteration, the interpolation equations have the form
	\begin{equation}
		Q_k(y_j)= F(y_j),\   j = 1, 2, ..., m.
	\end{equation}
	We let $x_k$ be the point in the set $\{y_j : j = 1, 2, ... , m\}$ that has the property
	\begin{equation}
		F(x_k)= min\ \{F(y_j), \  j = 1, 2, ..., m\}, 
	\end{equation}
	with any ties being broken by giving priority to an earlier evaluation of the least function value $F(x_k)$. A positive number $\Delta_k$, called the “trust region radius”, is also available at the beginning of the $k$-th iteration. If a termination condition\footnote{Typically, a termination condition is satified when the objective value can not be improved further after some iterations. for the termination condition of BOBYQA algorithm, please refer the paper \cite{MicPow09} for more details.} is satisfied, then the iteration stops. Otherwise, a step $d_k$ from $x_k$ is constructed such that $ \Vert d_k \Vert \leq \Delta_k $ holds, $x = x_k+d_k$ is within the bounds of equation \ref{DFO_bc}, and $x_k+d_k$ is not one of the interpolation points $y_j : j = 1, 2, ... , m$. Then the new function value $F(x_k+d_k)$ is calculated, and one of the interpolation points, $y_t$ say, is replaced by $x_k+d_k$, where $y_t$ is different from $x_k$. It follows that $x_{k+1}$ is defined by the formula
	\begin{equation}
		x_{k+1} =
		\begin{cases}
			x_k, & F(x_k+d_k) \geq F(x_k) \\
			x_k+d_k  , & F(x_k+d_k) < F(x_k) 
		\end{cases}
	\end{equation}
	
	Further, $\Delta_{k+1}$ and $Q_{k+1}$ are generated for the next iteration, $Q_{k+1}$ being subject to the constraints 
	\begin{equation}
		Q_{k+1}(\hat{y}_j)= F(\hat{y}_j), \  j = 1, 2, ..., m, 
	\end{equation}
	at the new interpolation points
	\begin{equation}
		\hat{y}_j =
		\begin{cases}
			y_j, & j \neq t, \\
			x_k+d_k  , & j =t 
		\end{cases},  \  j = 1, 2, ..., m.
	\end{equation}
	
	The operations of BOBYQA algorithm requires the user to provide an initial vector of variables $x_0 \in \mathcal{R}^n$, the initial trust region $\Delta_1$, and the number $m$ of interpolation conditions where $n+2 \leq m \leq \frac{1}{2}(n+1)(n+2)$. Two different ways have been proposed for constructing the step $d_k$ from $x_k$ and updating procedures from the $k$-th iteration to the $k+1$-th iteration in the paper \cite{MicPow09}, with both methods having utilized the "quadratic" nature of the approximation function $Q(\cdot)$.
	
	The lower level OCP of the Training Approach \ref{TA_rc} can be solved with the BOBYQA algorithm for a given $p$, since the lower level problem can be re-written into the form of \ref{DFO_bc} and the constraints are box-shaped. With the BOBYQA algorithm computing local extrema, the upper level problem still needs to be solved globally. In our rocket car case, this is straight-forward, i.e. maximizing over all $p$. 
	
	Nevertheless, the BOBYQA algorithm has limitations with several strong assumptions being made. Firstly, it has been assumed the uncertainty set is of moderate size and is box-shaped. Secondly, it has been assumed that there is only one local extrema, i.e. the lower level problem has only one solution for each $p \in \Omega_P$. In general, we cannot expect the second assumption to be valid. The BOBYQA algorithm is a gradient-free method with repect to the objective function $F(\cdot)$, it still utilizes the gradient of the approximation function $Q(\cdot)$ while updating the iteration. Therefore, this BOBYQA algorithm, or a general DFO approach, is still subject to the numerical errors and computational costs while calculating the gradients of the approximation function $Q(\cdot)$  and updating them in each iteration.  
	
	The paper at hand, instead, is utilizing the gradient of the objective function $F(\cdot)$ directly, with some approximation applied as well. We have used the multiple shooting and quasi-Newton method for solving the lower level problem of the Training Approach \ref{TA_rc}. In the chapter that follows, we introduce the quasi-Newton and multiple shooting method. 
	
	
	\chapter{quasi-Newton and Multiple Shooting Method}
	In this chapter, we first introduce the classical Newton method used for solving optimization problems and then focus on the quasi-Newton and multiple shooting method. 
	
	As stated in the introduction chapter \ref{Chapter1}, a general optimization problem is typically of the form 
	\begin{equation}
		\begin{aligned}
			\  \  \ & min \  f(x) \\
			s.t.\ \  & x \in \Omega
		\end{aligned}
		\label{OptGen}
	\end{equation}
	Here $x \in \Omega$ represents the constraints for which $x$ must satisfy, it may be in the form of $\{g(x) = 0,  h(x)  \leq  0 \}$ as in the problem \ref{GeneralMin}, i.e. the feasible set $\Omega = \underset{x}{arg} \ \{ g(x) = 0,  h(x)  \leq  0 \}$. Various methods exist for handling the constraints, such as  Karush–Kuhn–Tucker (KKT) method discussed in Section \ref{Sec_KKT}. After transforming the constrained problem into an unconstrained form, we can, therefore, directly apply algorithms that are suitable for unconstrained minimization. For the sake of simplicity, we focus on explaining the Newton and quasi-Newton method without constraints in this chapter. How the quasi-Newton can be expanded for problems with constraints, will be explained in the more details in Section \ref{Sec_KKT}. 
	
	\section{Newton method}
	The problem \ref{OptGen} without constraints, i.e. $min \  f(x)$  can be solved via Newton's method, which attempts to solve this problem by constructing a sequence $\{x_k\}$ from an initial guess (starting point) $x_0$ that converges towards a minimizer $x^\star$ of $f(x)$  by using a sequence of second-order Taylor approximations of $f(x)$ around the iterates. The second-order Taylor expansion of $f(x)$ around $x_k$ is
	%\begin{multline*}
	\begin{align*}
		f(x_k + \delta_x) \approx h(x_k) : = f(x_k) + f'(x_k)\delta(x_k) +\frac{1}{2}f''(x_k)\delta(x_k)^2 
	\end{align*}
	where $\delta$ represents a small change (with respect to $x$), and $f', f''$ are the first and second order derivatives of the original function $f(x)$. The notation $f', f''$ are usually expressed as $\nabla f$ and  $H$ (the Hessian matrix) respectively when $x$ is a vector of variables. In the text that follows, we will use the symbol $\nabla f$ and $H$ directly. Therefore, the Talyor expansion can be written as 
	\begin{align*}
		f(x_k + \delta_x) \approx h(x_k) : = f(x_k) + \nabla f(x_k)^T\delta(x_k) +\frac{1}{2}H(x_k)\delta(x_k)^2 
	\end{align*}
	The next iterate $x_{k+1}$ is defined so as to minimize this quadratic approximation $h(\cdot)$. The function $h(\cdot)$ is a quadratic function of $\delta(x)$, and is minimized by solving $\nabla h(\cdot) = 0$. The gradient of $h(\cdot)$ with respect to $\delta(x_k)$ at point $x_k$ is
	\begin{align*}
		\nabla h(x_k) = \nabla f(x_k) + H(x_k) \delta(x_k) 
	\end{align*}
	We are motivated to solve $\nabla h(x_k) =0$, which turns out to solve a linear system
	%\begin{align*}
	\begin{equation}
		\nabla f(x_k) + H(x_k) \delta(x_k) =0
		\label{HessianEq}
	\end{equation}
	%\end{align*}
	Therefore, for the next iteration point $x_{k+1}$, we can just add the small change $\delta(x_k)$ to the current iterate, i.e. 
	\begin{align*}
		x_{k+1}  = x_k + \delta(x_k) = x_k - H^{-1}(x_k)\nabla f(x_k), 
	\end{align*}
	here $ H^{-1}(\cdot)$ represents the inverse of the Hessian matrix $H(\cdot)$. The Newton method performs the iteration until the convergence, i.e. $x_k$ and $f(x_k)$ converge to $x^\star$ and $f(x^\star)$, respectively \footnote{In another word, the Newton mehtod has converged when the small change $\delta(x_k) =0$ or $\delta(x_k)$ is small enough that the change in the objective function is below a pre-defined tolerance level.}. The details of the Newton method is as follows: 
	\begin{description}
		\item[Newton method steps]\ 
		\begin{itemize}
			\item Step 0, $k=0$, choose an initial value $x_0$ 
			\item Step 1, $\delta(x_k)  =- H^{-1}(x_k)\nabla f(x_k)$, if $\delta(x_k) =0$, then stop
			\item Step 2, choose a step-size $\alpha_k$ (typically $\alpha_k =1$)
			\item Step 3, set $x_{k+1}  = x_k + \alpha_k \delta(x_k) $, let $k= k+1$. Go to Step 1
		\end{itemize}
	\end{description}
	
	The parameter $\alpha_k$ is introduced to augment the Newton method such that a line-search of $f(x_k + \alpha_k \delta(x_k))$ is applied to find an optimal value of the step size parameter $\alpha_k$. 
	
	Though the Newton method is straightforward and easy to understand, it has two main limitations. Firstly, it is sensitive to initial conditions. This is especially apparent if the objective function is non-convex. Depending on the choice of the starting point $x_0$, the Newton method may converge to a global minimum, a saddle point, a local minimum or may not converge at all. In another word, due to the sensitivity with respect to the initialization, the Newton method may be not able to find the global solution. Secondly, the Newton method can be computationally expensive, with the second-order derivatives, aka, the Hessian matrix $H(\cdot)$ and its inverse very expensive to compute. It may also happen that the Hessian matrix is not positive definite, therefore, Newton method can not be used at all for solving the optimization problem. Due to these limitations of the Newton method, instead, we have chosen the quasi-Netwon method for solving our rocket car problem. 
	
	\section{quasi-Newton method}
	We have stated that one limitation or the downside of the Newton method, is that Newton method can be computationally expensive when calculating the Hessian (i.e. second-order derivatives)  matrix and its inverse, especially when the dimensions get large. The quasi-Newton methods are a class of optimization methods that attempt to address this issue. More specifically, any modification of the Newton methods employing an approximation matrix $B$ to the original Hessian matrix $H$, can be classified into a quasi-Newton method. 
	
	The first quasi-Newton algorithm, i.e. the Davidon–Fletcher–Powell (DFP) method, was proposed by William C. Davidon in 1959 \cite{WilDav59}, which was later popularized by Fletcher and Powell in 1963 \cite{FlePow63}. Some of the most common used quasi-Newton algorithms currently are the symmetric rank-one (SR1) method \cite{ANP91} and the Broyden–Fletcher–Goldfarb–Shanno(BFGS) method. The family of the quasi-Newton algorithms are similar in nature, with most of the difference arising in the part how the approximation Hessian matrix is decided and the updating distance $\delta(x_k) $ is calculated. One of the main advantages of the quasi-Newton methods over Newton method is that the approximation Hessian matrix $B$ can be chosen in a way that no matrix needs to be directly inverted. The Hessian approximation $B$ is chosen to satisfy the equation \ref{HessianEq}, with the approximation matrix $B$ replacing the original Hessian matrix $H$, i.e. 
	\begin{equation}
		\nabla f(x_k) +B_k\delta(x_k) =0
		\label{HessianAppro}
	\end{equation}
	
	In the text that follows, we explain how the iteration is performed in the BFGS method, as an example illustrating the quasi-Newton method. In the BFGS method, instead of computing $B_k$ afresh at every iteration, it has been proposed to update it in a simple manner to account for the curvature measured during the most recent step. To determine an update scheme for $B$, we will need to impose additional constraints. One such constraint is the symmetry and positive-definiteness of $B$, which is to be preserved in each update for $k = 1,2, 3, ...$. Another desirable property is that $B_{k+1}$ is sufficiently close to $B_k$ at each update $k+1$, and such closeness can be measured by the matrix norm, i.e. the quantity $\Vert B_{k+1} - B_{k} \Vert$. We can, therefore, formulate our problem during the $k+1$ update as 
	\begin{equation}
		\begin{aligned}
			\underset{B_{k+1}}{min} \  &  \Vert B_{k+1} - B_{k} \Vert\\
			s.t.\ \  & B_{k+1}= B_{k+1}^T, \ B_{k+1}\delta(x_k)  = y_k \\
		\end{aligned}
		\label{BFGSB}
	\end{equation}
	where $\delta(x_k) = x_{k+1} -x_k$ and $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$. In the BFGS method, the norm is chosen to be the Frobenius norm:
	\begin{align*}
		\Vert B \Vert_F = \sqrt{\sum_{i}^{m} \sum_{j}^{n} |b_{ij}|^2} 
	\end{align*}
	Solving the problem \ref{BFGSB} directly is not trivial, but we can prove that problem ends up being equivalent to updating our approximate Hessian $B$ at each iteration by adding two symmetric, rank-one matrices $U$ and $V$:
	%Solving the problem \ref{BFGSBUp} directly is not trivial, but we can prove that problem ends up being equivalent to updating our approximate Hessian $B$ at each iteration by adding two symmetric, rank-one matrices $U$ and $V$:
	\begin{align*}
		B_{k+1} = B_k + U_k + V_k
	\end{align*}
	where the update matrices can then be chosen of the form $U = a u u^T$ and $V = b v v^T$, where $u$ and $v$ are linearly independent non-zero vectors, and $a$ and $b$ are constants.  The outer product of any two non-zero vectors is always rank one, i.e. $U_k$ and $V_k$ are rank-one. Since $u$ and $v$ are linearly independent, the sum of $U_k$ and $V_k$ is rank-two, and an update of this form is known as a rank-two update. The rank-two condition guarantees the “closeness” of $B_k$ and $B_{k+1}$ at each iteration. 
	
	Besides, the condition $B_{k+1}\delta(x_k) = y_k$ has to be imposed.
	\begin{align*}
		B_{k+1}\delta(x_k) = B_k\delta(x_k)  + a u u^T\delta(x_k) + b v v^T\delta(x_k) = y_k
	\end{align*}
	
	Then, a natural choice of $u$ and $v$ would be $u=y_k$ and $v=B_k\delta(x_k)$, we then have
	
	\begin{align*}
		B_k\delta(x_k) + a y_ky^T_k\delta(x_k) + bB_k\delta(x_k) \delta(x_k)^TB_k^T\delta(x_k) = y_k  \\
		y_k(1-ay_k^T\delta(x_k) ) = B_k\delta(x_k)(1+ b \delta(x_k)^TB_k^T\delta(x_k)) \\
		\Rightarrow a = \frac{1}{y_k^T\delta(x_k)}, \  b= - \frac{1}{\delta(x_k)^TB_k\delta(x_k)}
	\end{align*}
	Finally, we get the update formula as follows: 
	\begin{align*}
		B_{k+1} = B_k +  \frac{y_ky_k^T}{y_k^T\delta(x_k)}  - \frac{B_k\delta(x_k)\delta(x_k)^TB_k}{\delta(x_k)^TB_k\delta(x_k)}
	\end{align*}
	
	Since $B$ is positive definite for all $k = 1,2, 3, ...$, we can actually minimize the change in the inverse $B^{-1}$ at each iteration, subject to the (inverted) quasi-Newton condition and the requirement that it is symmetric. Applying the Woodbury formula, we can show (see the Appendix for more details) that the updating formula of inverse $B^{-1}$ is as follows
	
	\begin{equation}
		B_{k+1}^{-1} = (I - \frac{\delta(x_k)y_k^T}{y_k^T\delta(x_k)})B_k^{-1}(I - \frac{y_k\delta(x_k)^T}{y_k^T\delta(x_k)}) +  \frac{\delta(x_k)\delta(x_k)^T}{y_k^T\delta(x_k)} 
		\label{eq_updateB_}
	\end{equation}
	As shown in the formula \ref{eq_updateB_}, at each iteration, we update $B^{-1}$ by using  $\delta(x_k) = x_{k+1} -x_k$ and $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$. Since an update of $B^{-1}$ depends on the previous value, we need to initialize $B^{-1}$, either as an identity matrix or as the true Hessian matrix $H(x_0)$, calculated based on the starting point $x_0$.
	
	The problem \ref{TA_rc} involves constraints in the partial differential equation form and in a fixed time horizon (after the time transformation from $[0, T]$ to $t \in [0, 1]$). Therefore, we can apply multiple shooting method discretize the original OP problem into mutiple OP problems in different subintervals, with the constraints enforced at the boundary of subintervals to guarantee the continuity. Together with the matching conditions, we can then aggregate the subproblems and apply quasi-Newton method to get the final optimal solution. In the next section, we explain the multilpe shooting method first and in the next chapter we then focus on how the multilpe shooting and the quasi-Newton method, together, can be used for solving the rocket car problem.  
	
	\section{Multiple Shooting Method}
	Multiple shooting method was initially introduced to solve boundary value problem (BVP) in differential equation scope \cite{DJJ62}. This method, therefore, is well suited for solving optimal control problem with constraints in differential equations. However, some modification is necessary so that the multiple shooting method can be applied to solve optimal control problem. In the text that follows, we first explain how the multiple shooting can be used for solving BVP in the differential equation scope. After that, we explain how mutilple shooting can be applied to a general optimal control problem, and particularly how it can be used for solving the rocket car problem \ref{TA_rc}.
	
	To illustrate the concept of shooting method to solve boundary value problem (BVP), we use the the following example.
	\begin{align*}
		\dot{x} = x(t), t_0 \leq t \leq t_f	
	\end{align*}
	The analytical solution of above equation is 
	\begin{align*}
		x(t) = x(t_0)e^{t - t_0}
	\end{align*}
	where $e$ is the exponential number. Then $x(t_0) = x_0$ will be determined such that it will satisfy $x(t_f)=b$ for a given value $b$. Therefore, the equation $x(t_f)-b = 0$ or $x_0e^{t - t_0}-b =0$ is obtained. This derivation is called as shooting method. Generally, the shooting method can be summarized as follow
	\begin{description}
		\item[shooting method] \
		\begin{itemize}
			\item Step 1, choose an initial value $x_0 = x(t_0)$ 
			\item Step 2, form a solution of the differential equation from $t_0$ to $t_f$
			\item Step 3, evaluate the error at the boundary, if $x(t_f) - b = 0$, then stop, otherwise continue to Step 4 
			\item Step 4, update the guess for $x_0$ based on some updating schema, go to Step 2
		\end{itemize}
	\end{description}
	
	In multiple shooting method, the "shoot" interval is partitioned into some short intervals. We define a general differential equation with boundary value of the following form
	\begin{equation}\label{eqn:ori_dae}
		\begin{aligned}
			& \dot{y} = f(t, y, p) \\ 
			& y(t_f) = y_f  \\
		\end{aligned}
	\end{equation}
	where $y$ denotes the differential variables, $p$ is some parameter, $t \in [t_0, t_f]$,  $y_f$ is the boundary value at $t_f$.  With multiple shooting method, one chooses a suitable grid of multiple shooting nodes $\tau_j \in [t_0,t_f] $, where $t_0 = \tau_0 < \tau_1 < ... < \tau_m = t_f$,  i.e. $m$ subintervals covering the whole interval. At the beginning of each subinterval, $[\tau_k, \tau_{k+1}], k = 0, 1, ..., m-1$, we have the initial guess of the starting value $\hat{y}_k$. Then in each subinterval, we have the initial value problem of the following form: 
	\begin{equation}\label{eqn:msh}
		\begin{aligned}
			& \dot{y} = f(t, y, p) , t \in [\tau_k, \tau_{k+1}], \ k = 0, 1, ..., m-1   \\ 
			& y(\tau_k) = \hat{y}_k, k = 0, 1, ..., m-1  \\
		\end{aligned}
	\end{equation}
	In each subinterval, we introduce the new unkown parameter $\hat{y}_k$, we solve an initial value problem and will get a solution $y(t), t \in [\tau_k, \tau_{k+1}]$. The piecewise solution is not necessary continuous and also not necessarily satisfy the boundary condition $y(t_f) = y_f$. The continuity has to be enforced by additional matching conditions at each subinterval boundary, i.e. 
	\begin{equation}\label{eqn:mc}
		\begin{aligned}
			& y(\tau_{k+1}; \hat{y}_k) = \hat{y}_{k+1}, \  k = 0, 1, ..., m-1  \\
			& \hat{y}_{m} = \hat{y}_{\tau_m} = \hat{y}_{t_f} =  y_f 
		\end{aligned}
	\end{equation}
	
	The procedure of multiple shooting method can then be summarized as
	\begin{description}
		\item[Mutilple shooting method] \
		\begin{itemize}
			\item Step 1, choose multiple shooting nodes $t_0 = \tau_0 < \tau_1 < ... < \tau_m = t_f$ 
			\item Step 2, choose initial guess for $\hat{y}_k, k = 0, 1, ..., m-1$ 
			\item Step 3, form solutions of the differential equation in each subinterval $[\tau_k, \tau_{k+1}], k = 0, 1, ..., m-1$
			\item Step 4, evaluate the error at the boundary of each subinterval. If $y(\tau_{k+1}; \hat{y}_k) - \hat{y}_{k+1} = 0, \  k = 0, 1, ..., m-1$ and $\hat{y}_{m} - y_f =0$, then stop, otherwise continue to Step 5
			\item Step 5, update the guess for $\hat{y}_k, k = 0, 1, ..., m-1$ based on some updating schema, go to Step 3
		\end{itemize}
	\end{description}
	
	In practice, there are many details to be decided when using (multiple) shooting methods, which we will discuss briefly without giving a complete description. In Step 1 \& 2, when choosing the shooting nodes and the initial guess $\hat{y}_k$, how they are chosen usually depends on nature of the problem as well as a balance between accuracy and computational cost. For example, the nodes can be equally spaced and the $\hat{y}_k$ can be initialized with the same value, or they can be addressed in different methods. In Step 3, polynomial functions can be used as the approximate solutions, leveraging the fact that Taylor expansion can be used to approximate any continuous functions. In Step 4, "evaluate the error" is usually in the form of evaluating an objective function, which, e.g. can be defined as the sum of quadratic errors. In Step 5, the "updating schema" can be defined to so that the $\hat{y}_k$ can move in a direction that decreases the objective function. The (quasi-) Newton method, for example, can be used as an updating method in Step 5. 
	
	
	The multiple shooting method can be used for many problems. For example, it can be applied to the twice differential system of the following form 
	\begin{equation}
		y''(t) = f(t, y(t), y'(t))  \quad y(t_0) = y_0, \quad y(t_f) = y_f,  \quad t \in [t_0, t_f]
		\label{eqn:tdode}
	\end{equation}
	The problem \ref{eqn:tdode} is similar to the problem \ref{eqn:ori_dae}. In each subinterval, a boundary value problem (BVP) is to be solved and matching conditions at the boundary of each subinterval are to be enforced so that the final solution is continuous and applicable to the whole interval. 
	
	
	With the same idea, we can use mutiple shooting to solve optimal control problems. For the rocket car problem \ref{TA_rc}, we can discretize the time $t \in [0, 1]$ and the original problem is split into multiple subintervals, with the constraints discretized and applied to each subinterval. We also need to introduce the initial guess for each subinterval and add the matching condition at each boundary. In the end, we turn the original problem  \ref{TA_rc} into piecewise OCPs with augmented parameters and constraints. But the piecewise OCPs can be aggregated together due to their non-overlapping properties and the same structure, i.e. they can be aggregated to one objective function with the constraints expressed in matrix form. The transformed problem can then be solved with quasi-Newton method. In the next chatper, we discuss in detail how the problem \ref{TA_rc} can be solved with mutiple shooting and quasi-Newton method in details. 
	
	
	\chapter{Solving Rocket Car Problem}
	Before we dive into the mathematics, we first re-introduce the physics of the rocket car case so that we can describe and model the problem more precisely. 
	\section{Re-introduce the rocket car problem}
	Within the training approach, with a given $p$, we want to solve the lower level problem of the following form \ref{TA_lower2}
	\begin{subequations}
		\begin{align}
			\underset{}{min} \   & \  T \\ 
			s.t.  & \ \ x = (x_1, x_2)   \label{ta_rc_x} \\ 
			& \ \  \dot{x} = T  \begin{pmatrix}  x_2(t;p) \\ u(t)-p   \end{pmatrix}, & \ t \in [0,1],  \label{ta_rc_partial2} \\
			& \ \ x(0;p) = 0, \label{ta_rc_t2}\\
			& \ \ x_1(1;p) \geq 10, \label{ta_rc_x1_t2} \\
			& \ \ x_2(t;p) \leq 4, & t \in [0,1], \label{ta_rc_x2_tc2} \\
			& \ \ x_2(1;p) \leq 0, \label{ta_rc_x2_t1_2}  \\
			& \ \ T \geq 0, \\
			& \ \ u(t) \in [-10, 10], & t \in [0,1], \label{ta_ut}\\
			& \ \ p, \   a \ given \ value
		\end{align}
		\label{TA_lower2}
	\end{subequations}
	
	In summary, we want to find the minimum time that the rocket car moves from the starting state (the position is at the origin point, and the speed is zero) to an ending state(the position is at least at point 10 or beyond, and the speed is less than or equal to zero). A negative speed indicates the car is moving in a direction that decreases the position. During the whole process, constraints on the acceleration/deceleration value and the speed are to be satisfied. 
	
	Because our objective is to minimize the time between starting state and ending state, i.e. the variable $T$, which is unknown, we cannot define a time horizon over which we will discretize and optimize. Therefore, a new variable $t$ is defined as follows: 
	\begin{equation}
		t= \frac{\tau}{T} \in [0,1] \quad \tau \in [0, T]
		\label{eqn:timet}
	\end{equation}
	
	Where $\tau$ is the real time between starting time $0$ and ending time $T$, and $t$ is the relative time between $0$ and $1$.  The equation \ref{ta_rc_partial2} can be also written as 
	
	\begin{subequations}
		\begin{align}
			\dot{x} =  \begin{pmatrix} \dot{x_1} \\ \dot{x_2} \end{pmatrix}  & =  T  \begin{pmatrix}  x_2(t;p) \\ u(t)-p   \end{pmatrix} = \begin{pmatrix}  Tx_2(t;p) \\ T(u(t)-p)   \end{pmatrix} \label{eq_difT} \\ 
			\begin{pmatrix} \dot{x_1} \\ \dot{x_2} \end{pmatrix} &= \begin{pmatrix} \frac{\partial x_1}{\partial t} \\ \frac{\partial x_2}{\partial t} \end{pmatrix} = \begin{pmatrix} \frac{\partial x_1}{\partial \tau} \frac{\partial \tau}{\partial t} \\ \frac{\partial x_2}{\partial \tau} \frac{\partial \tau}{\partial t} \end{pmatrix} =  \begin{pmatrix} \frac{\partial x_1}{\partial \tau} T \\ \frac{\partial x_2}{\partial \tau}T \end{pmatrix} =     \begin{pmatrix}  Tx_2(t;p) \\ T(u(t)-p)   \end{pmatrix} \\
			\begin{pmatrix} \frac{\partial x_1}{\partial \tau}  \\ \frac{\partial x_2}{\partial \tau} \end{pmatrix} & =     \begin{pmatrix}  x_2(t;p) \\ u(t)-p   \end{pmatrix} \label{eq_difTau}
		\end{align}
		\label{partialX}
	\end{subequations}
	
	In summary, the equation $\frac{\partial x_1}{\partial \tau}= x_2(t;p) $ means the change in the position in real time is proportional to the speed at that moment. The equation $\frac{\partial x_2}{\partial \tau} = u(t)-p $ means the change in speed is proportional to the acceleration/deceleration value at that moment. 
	
	To use multiple shooting and quasi-Newton method, we discretize the interval $t\in [0,1]$ into subinterval $[t_{k}, t_{k+1}], k = 0, 1, 2, ..., m-1$, where $0 = t_0, t_1, t_2, ...,t_k, t_k, ..., t_m = 1$. We solve the OCP within each subinterval, and enforce the matching condition at the boundary of each subinterval. The equation \ref{eq_difT} is equivalent to \ref{eq_difTau}, and $\partial x_1= x_2(t;p) \partial \tau, \ \partial x_2 = ( u(t)-p) \partial \tau$. We can, therefore, solve the partial equations within each subinterval directly. We have the following solution
	
	\begin{subequations}
		\begin{align}
			x_1(t_{k+1}) -  x_1(t_k)  &= \int_{t_k}^{t_{k+1}} x_2(t;p) d \tau \label{eq_x1_int} \\
			x_2(t_{k+1}) -  x_2(t_k)  &= \int_{t_k}^{t_{k+1}} (u(t)-p) d \tau \\
			t \in [t_{k}, t_{k+1}], \  k &= 0, 1, 2, ..., m-1, \  t_0 =0, t_m =1
		\end{align}
	\end{subequations}
	
	The integral can be approximated by numerical method, one simple approximation method to the integral in equation \ref{eq_x1_int} can be 
	\begin{equation}
		\int_{t_k}^{t_{k+1}} x_2(t;p) d \tau  \approx \frac{x_2(t_{k+1}) + x_2(t_k)}{2} (\tau_{k+1} -\tau_k)
		\label{mid_approx}
	\end{equation}
	
	There are many methods that can be used to approximate the integrals, one of the widely used method is the Runge–Kutta method. With initial value $x_1(t_k),x_2(t_k), u(t_k)$ given at each subinterval $[t_{k}, t_{k+1}]$, based on the differential equation \ref{ta_rc_partial2}, we can find the (approximation) solution within this subinterval. 
	However, in order to get the feasible optimal solution for the original problem (i.e. the whole interval), we need to enforce the matching condition at the boundary of each subinterval as well as the constraints defined in equations \ref{ta_rc_t2}-\ref{ta_ut}.  These constraints can be addressed with the KKT method discussed in the next Section \ref{Sec_KKT}.
	
	
	\section{KKT Conditions}
	\label{Sec_KKT}
	In order to get the feasible solution, we can take the constraints into consideration by Lagrange multipliers with the Karush–Kuhn–Tucker (KKT) condition incorporated. Before we explain the KKT condition, we need to introduce several definitions and theorems first. We consider a general optimal control problem of the following form
	\begin{equation}
		\label{eq:OCP_discret_compact}
		\begin{aligned}
			\underset{x \in \mathbb{R}^n}{\text{min}} \qquad &f(x)	\\
			\qquad \text{s.t.}\qquad	&  g(x)	 = 0   \\
			&  h(x)	\leq 0 
		\end{aligned}
	\end{equation}
	\begin{definition}(Feasible set) The feasible set $\Omega$ is the set 
		\begin{align}
			\Omega:= \{x \in \mathbb{R}^n \ | \ g(x)= 0 , \ h(x)	\leq 0 \}	
		\end{align}
	\end{definition}	
	\begin{definition}(Active Constraints and Active Set) An inequality constraint $h_i(x) \leq 0$ is called active at $x^\star \in  \Omega$  iff  $h_i(x^\star) = 0$ and otherwise inactive. The index set $\mathcal{A}(x^\star) \subset \{1, ..., n_h\}$  of active inequality constraint indices is called the ”active set”.
	\end{definition}
	Often, the name active set also includes all equality constraint indices, as equalities could be considered to be always active. 
	\begin{definition} (LICQ) The linear independence constraint qualification (LICQ) holds at $x^\star \in  \Omega $ iff all vectors $\nabla g_i(x^\star)$ for $i \in \{1, ..., n_g \}$ and $\nabla h_i(x^\star)$ for  $i \in \mathcal{A}(x^\star)$ are linearly independent.
		\label{df_LICO}
	\end{definition}
	To give further meaning to the LICQ condition, let us combine all active inequalities with all equalities in a map $\tilde{g}$ defined by stacking all functions on top of each other in a colum vector as follows:
	\begin{equation}
		\tilde{g}(x) =  \begin{pmatrix} g(x) \\ h_i(x), \ i \in \mathcal{A}(x^\star)    \end{pmatrix}
	\end{equation}
	With the definitions above, we are ready to formulate the famous KKT condition. 
	\begin{theorem}(KKT Conditions)
		If $x^\star$ is a local minimizer of the problem \ref{TA_lower2} and the LICQ holds at $x^\star$, then there exist
		so called multiplier vectors $\lambda \in \mathbb{R}^n_g$ and $\mu \in \mathbb{R}^n_h$ with 
		\begin{subequations}
			\begin{align}
				\nabla f(x^\star) + \nabla g(x^\star) \lambda^\star +  \nabla h(x^\star) \mu^\star &= 0 \\
				g(x^\star)	 &= 0   \\
				h(x^\star)	&\leq 0  \label{kkt_smaller}\\
				\mu^\star & \geq 0 \\
				\mu_i^\star  h_i(x^\star) &=0 , \  i = 1, ..., n_h \label{kkt_active}
			\end{align}
		\end{subequations}
		\label{TH_KKT}
	\end{theorem}
	The "KKT Conditons" are also known as "First-Order Necessary Conditons". 
	
	\begin{definition}(KKT Point)
		We call a triple $(x^\star, \lambda^\star; \mu^\star)$ a "KKT Point" if it satisfies LICQ  (Definition \ref{df_LICO}) and the KKT conditions (Theorem \ref{TH_KKT}).
	\end{definition}
	\begin{definition}(Lagrangian Function)
		We define the so called "Lagrangian function" to be
		\begin{equation}
			\mathcal{L}(x,\lambda, \mu) = f(x) + \lambda^\top g(x) +  \mu^\top h(x) 
			\label{eq_Lagrangian}
		\end{equation}
	\end{definition}
	Here, we have used the so called "Lagrange multipliers" $\lambda \in \mathbb{R}^n_g$ and $\mu \in \mathbb{R}^n_h$. 
	The last three KKT conditions \ref{kkt_smaller} - \ref{kkt_active} are called the complementarity conditions. For each index $i$, they define an $L-$shaped set in the $(h_i, \mu_i)$ space. This set is not a smooth manifold but has a non-differentiability at the origin, i.e., if  $h_i(x^\star)=0$ and also $\mu_i^\star = 0$. This case is called a weakly active constraint. Often we want to exclude this case. On the other hand, an active constraint with $\mu_i^\star = 0$ is called strictly active.
	\begin{definition}
		Regard a KKT point $(x^\star, \lambda^\star; \mu^\star)$. We say that strict complementarity holds at this KKT point iff all	active constraints are strictly active.
	\end{definition}
	
	\begin{theorem}(Second Order Optimality Conditions) Let us regard a point $x^\star$ at which LICQ holds together with
		multipliers $\lambda^\star, \mu^\star$ so that the KKT conditions are satisfied and let strict complementarity hold. Regard a basis matrix $\mathbb{Z} \in  \mathbb{R}^{n*(n-n_g)}$ of the null space of $\frac{\partial \tilde{g}}{\partial x} (x^\star) \in \mathbb{R}^{n_{\tilde{g}} *n}$, i.e., $\mathbb{Z}$ has full column rank and $\frac{\partial \tilde{g}}{\partial x} (x^\star)\mathbb{Z} =0$. Then the following two statements hold:
	\end{theorem}
	\begin{itemize}
		\item  If $x^\star$ is a local minimizer, then $\mathbb{Z}^\top \nabla_x^2 \mathcal{L}(x^\star, \lambda^\star, \mu^\star)\mathbb{Z} \succeq 0  $.(Second Order Necessary Condition, short : SONC)
		\item  If $\mathbb{Z}^\top \nabla_x^2 \mathcal{L}(x^\star, \lambda^\star, \mu^\star)\mathbb{Z} \succ 0 $, then $x^\star$ a local minimizer. This minimizer is unique in its neighborhood, i.e., a strict local minimizer, and stable against small differentiable perturbations of the problem data. (Second Order Sufficient Condition, short: SOSC).
	\end{itemize}
	
	
	The matrix $\nabla_x^2 \mathcal{L}(x^\star, \lambda^\star, \mu^\star)$ plays an important role in optimization algorithms and is called the Hessian of the Lagrangian, while its projection on the null space of the Jacobian, $\mathbb{Z}^\top \nabla_x^2 \mathcal{L}(x^\star, \lambda^\star, \mu^\star)\mathbb{Z}$, is called the reduced
	Hessian. For an optimization problem, if we can start with an initial guess $x_0$ and find a updating schema which decreases the objective value while maintain the Hessian matrix positive, if such updating schema can converge, it will converge to a local minimizer. And if the original problem is convex, the local miminizer is, therefore, the global minimizer. Taking advantage of these properties, we can reformulate our rocket car problem into the form as in equation \ref{eq:OCP_discret_compact}, and then re-write in the Lagrangian form \ref{eq_Lagrangian}, optimize the (objective) Lagrangian function with quasi-Newton method. 
	
	\section{Rokect car problem in Lagrangian form}
	
	We discretize the original problem \ref{TA_lower2} based on time $t$ into $m$ sub-problems, i.e. we split the interval $t\in [0,1]$ into subinterval $[t_{k}, t_{k+1}],\  k = 0, 1, 2, ..., m-1$, where $0 = t_0, t_1, t_2, ...,t_k, t_{k+1}, ..., t_m = 1$. We assume the parameter $p$ is given, for example, we can assume $p=0$, and find a corresponding optimal $T$. Later, we can find different optimal $T$ with different $p$ given. In each subinterval, we have inital guess for $x$, we introduce anther two variables, with $s$ as the guess for $x$, and $q$ as the guess for $u$. Therefore, in each subinterval, we have the initial value $ x(t_k) = s(t_k), u(t_k) = q(t_k),  k = 0, 1, 2, ..., m-1$. When no confusion arises, we will write $s(t_k) = s_k $ and $q(t_k) = q_k$. Notice $s(t_k)$ has two component which corrsponds to $x_1$ (position) and $x_2$ (speed). When we want to specify the exact component at time $t_k$, we would write $s(t_k) = (s_1(t_k), s_2(t_k))$. We would like to find the solution for $x(t; q_k, s_k, p),  t  \in [t_{k}, t_{k+1}]$. Assume that such solution has been found, the matching condition at the boundary of each sub-interval will lead to the following equation
	
	\begin{equation}
		g(s,q) = \begin{bmatrix}
			s_0 -0 \\
			x(t_0; p) - s_0  \\
			x(t_1; t_0, s_0, q_0, p) - s_1 \\ 
			\vdots \\
			x(t_{m-1}; t_{m-2}, s_{m-2}, q_{m-2}, p) - s_{m-1} \\
		\end{bmatrix} = 0
		\label{TA_eqCons}
	\end{equation}
	Notice, we have the equality constraints up to the second to the last subinterval, i.e. $[t_{m-2}, t_{m-1}]$. This is because the terminal condition at time $t_m =1$ (which the subinterval $[t_{m-1}, t_m]$ covers) has already been defined in the inequality constraints \ref{ta_rc_x1_t2} and \ref{ta_rc_x2_t1_2}. Moreover, the boundary point $s_k, u_k, k = 0, 1, 2, ..., m-1$ is also subject to the inequality constraints \ref{ta_rc_x2_tc2} and \ref{ta_ut}. All these constraints can be combined together in the following form
	
	\begin{equation}
		h(s,q) = \begin{bmatrix}
			s_2(t_k) - 4, k =0, 1, 2, ..., m-1  \\
			10 - x_1(t_m; t_{m-1}, s_{m-1}, q_{m-1}, p)\\ 
			x_2(t_m; t_{m-1}, s_{m-1}, q_{m-1}, p) \\
			q_k - 10,   k =0, 1, 2, ..., m-1 \\
			-10 - q_k,  k =0, 1, 2, ..., m-1 \\
			u(t_m) - 10 \\
			-10 - u(t_m) \\
		\end{bmatrix} \leq 0
		\label{TA_IneqCons}
	\end{equation}
	
	Within equations \ref{TA_eqCons} and \ref{TA_IneqCons}, the solution of each subinterval 
	\begin{equation}
		\begin{aligned}
			x(t; w_k) &= (x_1(t;w_k), x_2(t; w_k)),  t \in [t_{k}, t_{k+1}]\\
			w_k &= (s_k, q_k, p) \\
			p, & \   a \ given \ value
		\end{aligned}
	\end{equation}
	comes from numerically solving the partial differential equation \ref{partialX}. For example, using the Runge–Kutta method or simply using the approximation as used in equation \ref{mid_approx}. 
	
	The original objective function  $min \ T$ is to minimize the free final time, which is unknown, and we are discretizing the variable $t \in [0,1]$ as defined in equation \ref{eqn:timet}. Then the original problem can be transformed, with the final time $T$ as a variable to be optimized. Therefore, we can regard the triple $(s,q,T):= w$ as the new independent variables, and the original problem \ref{TA_lower2} is then transformed into the following form
	\begin{equation}
		\label{TA_Transform}
		\begin{aligned}
			\underset{w}{\text{min}} \qquad & 1	\\
			\qquad \text{s.t.}\qquad	&  g(w) = 0   \\
			&  h(w)	\leq 0  \\ 
			w &= (s, q, T)
		\end{aligned}
	\end{equation}
	The objective function actually is gone, only the constraints play a role. Therefore, we have the Lagrangian function for the rocket car problem of the following form
	\begin{equation}
		\mathcal{L}(w,\lambda, \mu) = T(w) + \lambda^\top g(w) +  \mu^\top h(w) 
		\label{eq_RC_Lag}
	\end{equation}
	
	
	
	
	%% The objectitve function $min \ T$ can be regarded as a function of $T(s, q)$.
	
	
