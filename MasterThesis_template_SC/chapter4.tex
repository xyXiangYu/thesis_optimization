\chapter{The Introduction to Multiple Shooting and Quasi-Newton Method}
In this chapter, we first introduce the basics of solutions to optimization problems and the focus on the quasi-Newton and multiple shooting method. 
As stated in the introduction chapter \ref{Chapter1}, a general optimization problem is typically of the form 
\begin{equation}
	\begin{aligned}
		\  \  \ & min \  f(x) \\
		s.t.\ \  & x \in \Omega
	\end{aligned}
	\label{OptGen}
\end{equation}
Here $x \in \Omega$ represents the constraints for which $x$ must satisfy, it may be in the form of $ g(x) = 0,  h(x)  \geq  0$ as in the problem \ref{GeneralMin} in the introduction chapter \ref{Chapter1}, i.e. the feasible set $\Omega = \underset{x}{arg} \ \{ g(x) = 0,  h(x)  \geq  0 \}$. 

\section{Newton method}
Such problem \ref{OptGen} can be solved via Newton's method, which attempts to solve this problem by constructing a sequence $\{x_k\}$ from an initial guess (starting point) $x_0 \in \Omega$ that converges towards a minimizer $x^\star$ of $f(x)$  by using a sequence of second-order Taylor approximations of $f(x)$ around the iterates. The second-order Taylor expansion of $f(x)$ around $x_k$ is
%\begin{multline*}
\begin{align*}
f(x_k + \delta_x) \approx h(x) : = f(x_k) + f'(x_k)\delta x +\frac{1}{2}f''(x_k)\delta_x^2 
\end{align*}
where $\delta_x$ represents a small change (with respect to $x$), and $f', f''$ are the first and second order derivatives of the original function $f(x)$. $f''$ is also called Hessian matrix of $f(x)$ when $x$ is a vector, i.e. having multiple variables. The next iterate $x_{k+1}$ is defined so as to minimize this quadratic approximation $h(x)$. Since $h(x)$ is a quadratic function, and is minimized by solving $h'(x) = 0$. Since the gradient of $h(x)$ is at point $x_k$ is :
\begin{align*}
h'(x_k) = f'(x_k) +f''(x_k)\delta_x 
\end{align*}
Therefore, we are motivated to solve $\nabla  h(x_k) =0$, which yields
\begin{align*}
	\delta_x  = - \frac{ f'(x_k)}{f''(x_k)}
\end{align*}
Therefore, for the next iteration point $x_{k+1}$, we can just add the small change $\delta_x$ to the current iterate, i.e. 
\begin{align*}
	x_{k+1}  = x_k + \delta_x = x_k - \frac{ f'(x_k)}{f''(x_k)}
\end{align*}

The Newton's method performs the iteration until the convergence, i.e. $x_k$ and $f(x_k)$ converge to $x^\star$ and $f(x^\star)$, respectively \footnote{In another word, the Newton mehtod has converged when the small change $\delta_x =0$ or $\delta_x$ is small enough that the change in the objective function is below a pre-defined tolerance level.}. The details of the Newton method is as follows: 
\begin{description}
	\item[Newton method steps]\ 
	\begin{itemize}
		\item Step 0, $k=0$, choose an initial value $x_0 \in \Omega$ 
		\item Step 1, $\delta_x  = - \frac{ f'(x_k)}{f''(x_k)}$, if $\delta_x =0$, then stop
		\item Step 2, choose a step-size $\alpha_k$ (typically $\alpha_k =1$
		\item Step 3, set $x_{k+1}  = x_k + \alpha_k \delta_x $, let $k= k+1$. Go to Step 1
	\end{itemize}
\end{description}

The parameter $\alpha_k$ is introduced to augment the Newton method such that a line-search of $f(x_k + \alpha_k \delta_x)$ is applied to find an optimal value of the step size parameter $\alpha_k$. 

Though the Netwon method is straightforward and easy to understand, it has two main limitations. Firstly, it is sensitive to initial conditions. This is especially apparent if our objective function is non-convex. Depending on the choice of the starting point $x_0$, the Newton method may converge to a saddle point, a local minimum or may not converge at all. In these cases, the Newton method can not find the global solution. Secondly, the Newton method can be computationally expensive, with the second-order derivatives $f''(x)$, aka, the Hessian matrix very expensive to compute. It may also happen that the Hessian matrix is not positive definite, therefore, Newton method can not be used at all for solving the optimization problem. Due to these limitations of the Newton method, we have chosen the quasi-Netwon method instead for our problem in the rocket car case. 

\section{Quasi-Newton method}
We have stated that one limitation or the downside of the Newton method, is that Newton method can be computationally expensive when calculating the Hessian (i.e. second-order derivatives)  matrix and its invere, especially when dimensions get large. Quasi-Newton methods are a class of optimization methods that attempt to address this issue. More specifically, Quasi-Newton methods are used to find the global minimum of a function $f(x)$ that is twice-differentiable. 



\begin{equation} 
	A = 
	\begin{pmatrix}
		1+2\lambda \theta  & -\lambda \theta  & \cdots & 0 \\
		-\lambda \theta  & 1+2\lambda \theta & \cdots & 0\\
		\vdots  & \vdots  & \ddots & \vdots  \\
		0 & 0 & \cdots & 1+2\lambda \theta
	\end{pmatrix}
\end{equation}

