\chapter{quasi-Newton and Multiple Shooting Method}
In this chapter, we first introduce the classical Newton method used for solving optimization problems and then focus on the quasi-Newton and multiple shooting method. 

As stated in the introduction chapter \ref{Chapter1}, a general optimization problem is typically of the form 
\begin{equation}
	\begin{aligned}
		\  \  \ & min \  f(x) \\
		s.t.\ \  & x \in \Omega
	\end{aligned}
	\label{OptGen}
\end{equation}
Here $x \in \Omega$ represents the constraints for which $x$ must satisfy, it may be in the form of $ g(x) = 0,  h(x)  \geq  0$ as in the problem \ref{GeneralMin}, i.e. the feasible set $\Omega = \underset{x}{arg} \ \{ g(x) = 0,  h(x)  \geq  0 \}$. Various methods exist for eliminating the constraints, therefore, we can solve the resulting problem with algorithms for unconstrained minimization. For the sake of simplicity, we focus on explaining the Newton and quasi-Newton method without constraints in this chapter. How the quasi-Newton can be expanded for problems with constraints, will be explained in the next chapter. 


\section{Newton method}
The problem \ref{OptGen} without constraints, i.e. $min \  f(x)$  can be solved via Newton's method, which attempts to solve this problem by constructing a sequence $\{x_k\}$ from an initial guess (starting point) $x_0$ that converges towards a minimizer $x^\star$ of $f(x)$  by using a sequence of second-order Taylor approximations of $f(x)$ around the iterates. The second-order Taylor expansion of $f(x)$ around $x_k$ is
%\begin{multline*}
\begin{align*}
f(x_k + \delta_x) \approx h(x_k) : = f(x_k) + f'(x_k)\delta(x_k) +\frac{1}{2}f''(x_k)\delta(x_k)^2 
\end{align*}
where $\delta$ represents a small change (with respect to $x$), and $f', f''$ are the first and second order derivatives of the original function $f(x)$, and are usually denoted as $\nabla f$ and the Hessian matrix $H$ of $f(x)$ respectively when $x$ is a vector of multiple variables. In the text that follows, we will use the symbol $\nabla f$ and $H$ directly, therefore, the Talyor expansion can be written as 
\begin{align*}
	f(x_k + \delta_x) \approx h(x_k) : = f(x_k) + \nabla f(x_k)^T\delta(x_k) +\frac{1}{2}H(x_k)\delta(x_k)^2 
\end{align*}
 The next iterate $x_{k+1}$ is defined so as to minimize this quadratic approximation $h(\cdot)$. The function $h(\cdot)$ is a quadratic function of $\delta(x)$, and is minimized by solving $\nabla h(\cdot) = 0$. The gradient of $h(\cdot)$ with respect to $\delta(x_k)$ at point $x_k$ is
\begin{align*}
\nabla h(x_k) = \nabla f(x_k) + H(x_k) \delta(x_k) 
\end{align*}
We are motivated to solve $\nabla h(x_k) =0$, which leads to solve a linear system
%\begin{align*}
\begin{equation}
\nabla f(x_k) + H(x_k) \delta(x_k) =0
	\label{HessianEq}
\end{equation}
%\end{align*}
Therefore, for the next iteration point $x_{k+1}$, we can just add the small change $\delta(x_k)$ to the current iterate, i.e. 
\begin{align*}
	x_{k+1}  = x_k + \delta(x_k) = x_k - H^{-1}(x_k)\nabla f(x_k), 
\end{align*}
here $ H^{-1}(\cdot)$ represents the inverse of the Hessian matrix $H(\cdot)$. The Newton's method performs the iteration until the convergence, i.e. $x_k$ and $f(x_k)$ converge to $x^\star$ and $f(x^\star)$, respectively \footnote{In another word, the Newton mehtod has converged when the small change $\delta(x_k) =0$ or $\delta(x_k)$ is small enough that the change in the objective function is below a pre-defined tolerance level.}. The details of the Newton method is as follows: 
\begin{description}
	\item[Newton method steps]\ 
	\begin{itemize}
		\item Step 0, $k=0$, choose an initial value $x_0$ 
		\item Step 1, $\delta(x_k)  =- H^{-1}(x_k)\nabla f(x_k)$, if $\delta(x_k) =0$, then stop
		\item Step 2, choose a step-size $\alpha_k$ (typically $\alpha_k =1$)
		\item Step 3, set $x_{k+1}  = x_k + \alpha_k \delta(x_k) $, let $k= k+1$. Go to Step 1
	\end{itemize}
\end{description}

The parameter $\alpha_k$ is introduced to augment the Newton method such that a line-search of $f(x_k + \alpha_k \delta(x_k))$ is applied to find an optimal value of the step size parameter $\alpha_k$. 

Though the Newton method is straightforward and easy to understand, it has two main limitations. Firstly, it is sensitive to initial conditions. This is especially apparent if the objective function is non-convex. Depending on the choice of the starting point $x_0$, the Newton method may converge to a global minimum, a saddle point, a local minimum or may not converge at all. In another word, due to the sensitivity with respect to the initialization, the Newton method may be not able to find the global solution. Secondly, the Newton method can be computationally expensive, with the second-order derivatives, aka, the Hessian matrix $H(\cdot)$ and its inverse very expensive to compute. It may also happen that the Hessian matrix is not positive definite, therefore, Newton method can not be used at all for solving the optimization problem. Due to these limitations of the Newton method, we have chosen the quasi-Netwon method instead for solving our problem in the rocket car case. 

\section{quasi-Newton method}
We have stated that one limitation or the downside of the Newton method, is that Newton method can be computationally expensive when calculating the Hessian (i.e. second-order derivatives)  matrix and its inverse, especially when the dimensions get large. The quasi-Newton methods are a class of optimization methods that attempt to address this issue. More specifically, any modification of the Newton methods employing an approximation matrix $B$ to the original Hessian matrix $H$, can be classified into a quasi-Newton method. 

The first quasi-Newton algorithm, i.e. the Davidon–Fletcher–Powell (DFP) method, was proposed by William C. Davidon in 1959 \cite{WilDav59}, which was later popularized by Fletcher and Powell in 1963 \cite{FlePow63}. Some of the most common used quasi-Newton algorithms currently are the symmetric rank-one (SR1) method \cite{ANP91} and Broyden–Fletcher–Goldfarb–Shanno(BFGS) method. The family of the quasi-Newton algorithms are similar in nature, with most of the difference arising in the part how the approximation Hessian matrix is decided and the updating distance $\delta(x_k) $ is calculated. One of the chief advantages of the quasi-Newton methods over Newton's method is that the approximation Hessian matrix $B$ can be chosen in a way that no matrix needs to be directly inverted. The Hessian approximation $B$ is chosen to satisfy the equation \ref{HessianEq}, with the approximation matrix $B$ replacing the original Hessian matrix $H$, i.e. 
\begin{equation}
	\nabla f(x_k) +B_k\delta(x_k) =0
	\label{HessianAppro}
\end{equation}

In the text that follows, we explain how the iteration is performed in the BFGS method, as an example illustrating the quasi-Newton method. In the BFGS method, instead of computing $B_k$ afresh at every iteration, it has been proposed to update it in a simple manner to account for the curvature measured during the most recent step. To determine an update scheme for $B$, we will need to impose additional constraints. One such constraint is the symmetry and positive-definiteness of $B$, which is to be preserved in each update for $k = 1,2, 3, ...$. Another desirable property we want is for $B_{k+1}$ to be sufficiently close to $B_k$ at each update $k+1$, and such closeness can be measured by the matrix norm, i.e. the quantity $\Vert B_{k+1} - B_{k} \Vert$. We can, therefore, formulate our problem during the $k+1$ update as 
\begin{equation}
	\begin{aligned}
		 \underset{B_{k+1}}{min} \  &  \Vert B_{k+1} - B_{k} \Vert\\
		s.t.\ \  & B_{k+1}= B_{k+1}^T, \ B_{k+1}\delta(x_k)  = y_k \\
	\end{aligned}
	\label{BFGSB}
\end{equation}
where $\delta(x_k) = x_{k+1} -x_k$ and $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$. In the BFGS method, the norm is chosen to be the Frobenius norm:
\begin{align*}
\Vert B \Vert_F = \sqrt{\sum_{i}^{m} \sum_{j}^{n} |b_{ij}|^2} 
\end{align*}
Solving the problem \ref{BFGSB} directly is not trivial, but we can prove that problem ends up being equivalent to updating our approximate Hessian $B$ at each iteration by adding two symmetric, rank-one matrices $U$ and $V$:
%Solving the problem \ref{BFGSBUp} directly is not trivial, but we can prove that problem ends up being equivalent to updating our approximate Hessian $B$ at each iteration by adding two symmetric, rank-one matrices $U$ and $V$:
\begin{align*}
 B_{k+1} = B_k + U_k + V_k
\end{align*}
where the update matrices can then be chosen to be of the form $U = a u u^T$ and $V = b v v^T$, where $u$ and $v$ are linearly independent non-zero vectors, and $a$ and $b$ are constants.  The outer product of any two non-zero vectors is always rank one, i.e. $U_k$ and $V_k$ are rank-one. Since $u$ and $v$ are linearly independent, the sum of $U_k$ and $V_k$ is rank-two, and an update of this form is known as a rank-two update. The rank-two condition guarantees the “closeness” of $B_k$ and $B_{k+1}$ at each iteration. 

Besides, the condition $B_{k+1}\delta(x_k) = y_k$ has to be imposed.
\begin{align*}
	B_{k+1}\delta(x_k) = B_k\delta(x_k)  + a u u^T\delta(x_k) + b v v^T\delta(x_k) = y_k
\end{align*}

Then, a natural choice of $u$ and $v$ would be $u=y_k$ and $v=B_k\delta(x_k)$, we then have

\begin{align*}
	B_k\delta(x_k) + a y_ky^T_k\delta(x_k) + bB_k\delta(x_k) \delta(x_k)^TB_k^T\delta(x_k) = y_k  \\
	y_k(1-ay_k^T\delta(x_k) ) = B_k\delta(x_k)(1+ b \delta(x_k)^TB_k^T\delta(x_k)) \\
	\Rightarrow a = \frac{1}{y_k^T\delta(x_k)}, \  b= - \frac{1}{\delta(x_k)^TB_k\delta(x_k)}
\end{align*}
Finally, we get the update formula as follows: 
\begin{align*}
	B_{k+1} = B_k +  \frac{y_ky_k^T}{y_k^T\delta(x_k)}  - \frac{B_k\delta(x_k)\delta(x_k)^TB_k}{\delta(x_k)^TB_k\delta(x_k)}
\end{align*}

Since $B$ is positive definite for all $k = 1,2, 3, ...$, we can actually minimize the change in the inverse $B^{-1}$ at each iteration, subject to the (inverted) quasi-Newton condition and the requirement that it be symmetric. Applying the Woodbury formula , we can show (see the Appendix for more details) that the updating formula of inverse $B^{-1}$ is as follows

\begin{align*}
	B_{k+1}^{-1} = (I - \frac{\delta(x_k)y_k^T}{y_k^T\delta(x_k)})B_k^{-1}(I - \frac{y_k\delta(x_k)^T}{y_k^T\delta(x_k)}) +  \frac{\delta(x_k)\delta(x_k)^T}{y_k^T\delta(x_k)} 
\end{align*}
As shown in the formula above, at each iteration, we update the value of $B^{-1}$ using only the values of $\delta(x_k) = x_{k+1} -x_k$ and $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$. Since an update of $B^{-1}$ depends on the previous value, we need to initialize $B^{-1}$, either as an identity matrix or as the true Hessian matrix $H(x_0)$, calculated based on the starting point $x_0$.

The problem \ref{TA_rc} involves constraints in the partial differential equation form. Therefore, we can apply multiple shooting method to transform the original problem into  discretized non-linear optimal control problem, with the constraints applied in each subinterval. Together with the matching conditions, we can then apply quasi  , apply multiple shooting and quasi-Newton method to get the optimal solution. In the text the follows, we explain the multilpe shooting method first and then focus on how it, together with quasi-Newton method, can be used for solving the rocket car problem.  

\section{Multiple Shooting Method}
Multiple shooting method was initially introduced to solve boundary value problem (BVP) in differential equation scope \cite{DJJ62}. This method, therefore, is well suited for solving optimal control problem which consists of differential equations with constraints. However, some modification is necessary so that the multiple shooting method can be applied to solve optimal control problem. In the text that follows, we first explain how the multiple shooting can be used for solving BVP in the differential equation scope. After that, we explain how mutilple shooting can be applied to a general optimal control problem, and particularly how it can be used for solving the rocket car case \ref{TA_rc}.

To illustrate the concept of shooting method to solve BVP, we use the the following example.
\begin{align*}
	\dot{x} = x(t), t_0 \leq t \leq t_f	
\end{align*}
The analytical solution of above equation is 
\begin{align*}
x(t) = x(t_0)e^{t - t_0}
\end{align*}
where $e$ is the exponential number. Then $x(t_0) = x_0$ will be determined such that it will satisfy
$x(t_f)=b$ for given value $b$. Therefore, the equation $x(t_f)-b = 0$ or $x_0e^{t - t_0}-b =0$ is obtained. This derivation is called as shooting method. Generally, the shooting method can be summarized as follow
\begin{description}
	\item[shooting method] \
	\begin{itemize}
		\item Step 1, choose an initial value $x_0 = x(t_0)$ 
		\item Step 2, form a solution of the differential equation from $t_0$ to $t_f$
		\item Step 3, evaluate an error of the boundary condition, if $x(t_f) - b = 0$, then stop, otherwise continue to Step 4 
		\item Step 4, update the guess for $x_0$ based on some updating schema, go to Step 2
	\end{itemize}
\end{description}

In multiple shooting method, the "shoot" interval is partitioned into some short intervals. We define a general differential equation with boundary value of the following form
\begin{equation}\label{eqn:ori_dae}
	\begin{aligned}
		& \dot{y} = f(t, y, p) \\ 
		& y(t_f) = y_f  \\
	\end{aligned}
\end{equation}
where $y$ denotes the differential variables, $p$ is some parameter, $t \in [t_0, t_f]$,  $y_f$ is the boundary value at $t_f$.  With multiple shooting method, one chooses a suitable grid of multiple shooting nodes $\tau_j \in [t_0,t_f] $, where $t_0 = \tau_0 < \tau_1 < ... < \tau_m = t_f$,  i.e. $m$ subintervals covering the whole interval. At the beginning of each subinterval, $[\tau_k, \tau_{k+1}], k = 0, 1, ..., m-1$, we have the initial guess of the starting value $\hat{y}_k$. Then in each subinterval, we have the initial value problem of the following form: 
\begin{equation}\label{eqn:msh}
	\begin{aligned}
		& \dot{y} = f(t, y, p) , t \in [\tau_k, \tau_{k+1}], \ k = 0, 1, ..., m-1   \\ 
		& y(\tau_k) = \hat{y}_k, k = 0, 1, ..., m-1  \\
	\end{aligned}
\end{equation}
In each subinterval, we introduce the new unkown parameter $\hat{y}_k$, we solve an initial value problem and will get a solution $y(t), t \in [\tau_k, \tau_{k+1}]$. The piecewise solution is not necessary continuous and also not necessarily satisfy the boundary condition $y(t_f) = y_f$. The continuity has to be enforced by additional matching conditions at each subinterval point, i.e. 
\begin{equation}\label{eqn:mc}
	\begin{aligned}
		& y(\tau_{k+1}; \hat{y}_k) = \hat{y}_{k+1}, \  k = 0, 1, ..., m-1  \\
		& \hat{y}_{m} = \hat{y}_{\tau_m} = \hat{y}_{t_f} =  y_f 
	\end{aligned}
\end{equation}

The procedure of multiple shooting method can then be summarized as
\begin{description}
	\item[Mutilple shooting method] \
	\begin{itemize}
		\item Step 1, choose multiple shooting nodes $t_0 = \tau_0 < \tau_1 < ... < \tau_m = t_f$ 
		\item Step 2, choose initial guess for $\hat{y}_k, k = 0, 1, ..., m-1$ 
		\item Step 3, form solutions of the differential equation in each subinterval $[\tau_k, \tau_{k+1}], k = 0, 1, ..., m-1$
		\item Step 4, evaluate the error at boundary of each interval condition, if $y(\tau_{k+1}; \hat{y}_k) - \hat{y}_{k+1} = 0, \  k = 0, 1, ..., m-1$ and $\hat{y}_{m} = y_f$, then stop, otherwise cotinue to Step 5
		\item Step 5, update the guess for $\hat{y}_k, k = 0, 1, ..., m-1$ based on some updating schema, go to Step 3
	\end{itemize}
\end{description}

In practice, there are many details to be decided when using (multiple) shooting methods, which we will introduce briefly without giving a rigorous formation. In Step 1 \& 2, when choosing the shooting nodes and the initial guess $\hat{y}_k$, how they are chosen usually depends on nature of the problem as well as a balance between accuracy and computational cost. For example, the nodes can be equally spaced and the $\hat{y}_k$ can be initialized with the same value, or they can be addressed in other methods. In Step 3, polynomial functions can be used as the approximate solutions, leveraging the fact that Taylor expansion can be used to approximate any continuous functions. In Step 4, "evaluate the error" is usually in the form of evaluating an objective function, which, e.g. can be defined as the sum of quadratic errors. In Step 5, the "updating schema" can be defined to so that the $\hat{y}_k$ can move in a direction that decreases the objective function. The (quasi) Newton method, for example, can be used as an updating method in Step 5. 


The multiple shooting method can be used for many problems. For example, it can be applied to the twice differential system of the following form 
\begin{equation*}
	 y''(t) = f(t, y(t), y'(t))  \quad y(t_0) = y_0, \quad y(t_f) = y_f,  \quad t \in [t_0, t_f]
	 \label{eqn:tdode}
\end{equation*}
The problem above is similar to the problem \ref{eqn:ori_dae}. In each subinterval, a boudary value problem (BVP) is to be solved and matching conditions at the boundary of each subinterval  are to be enforced applied so that the final solution found is applicable to the whole interval. 


With the same idea, we can use mutiple shooting to solve a optimal control problem. For the rocket car problem \ref{TA_rc}, we can discretize the time $t \in [0, 1]$ and the original OCP problem is split into multiple intervals, with the constraints also applied to each subinterval. We also need to introduce the initial guess for each interval and add the matching constraints. In the end, we turn the original problem  \ref{TA_rc} into piecewise OCPs with augmented parameters and constraints. But the piecewise OCPs can be solved as as a whole due to their non-overlapping properties and same structure, i.e. they can be aggregated to one objective function with the constraints expressed in matrix form. The final problem can then be solved with quasi-Newton method. In the next chatper, we discuss in detail how the problem \ref{TA_rc} can be solved with mutiple shooting method and quasi-Newton method in details. 








	%\begin{aligned}
	%	& y''(t) = f(t, y(t), y'(t))  \quad y(t_0) = y_0, \quad y(t_f) = y_f,  \quad t \in %[t_0, t_f] 
	%\end{aligned}
%\end{equation*}
% 


%the number of nodes to is usually decided by a balance of accuracy and computation cost. How the nodes is chosen (e.g. equally spaced or some other ways) depends on the nature of the problem itself. In Step 2, 
%The step of  in the (multiple) shooting methods is usually in the form of evaluating an objective function, which, e.g. can be defined as the sum of quadratic error. 



%and the solution must statisfy the matching condition
%The state variable $s_k, k = 0, 1, ..., m-1$ are newly introduced unknown parameters, thus the original DAE problem \ref{eqn:ori_dae} becomes $m$ relaxed DAE initial value problems (IVPs). With some initial guess of $s_k$ in their corresponding subinterval, we could solve the DAE IVP in that subinterval. The initial value problem solutions thus generated are neither continuous at $t_k$, nor do they satisfy the algebraic equations. This has to be enforced by additional matching conditions. 

%E.g. in subinterval $ [\tau_k, \tau_{k+1}]$ and $ [\tau_{k+1}, \tau_{k+2}]$, we have some intial values for $s_k$ and $s_{k+1}$ for each subinterval, and we have the solution  $y_k^*, z_k^*$ and $y_{k+1}^*, z_{k+1}^*$ for each subinterval as well. In order to have all the solutions in the subintervals matching with the real solution of $y, z$ in the whole domain of $[a, b]$, we must have that the solutions in the subintervals to be continuous at the boundaries of these subintervals. That is: 
%\begin{align*} 
%	y_k^*(\tau_{k+1}) &=  y_{k+1}^*(\tau_{k+1}) = s_{k+1}^y, \  k = 0, 1, ..., m-2  \\
%	z_k^*(\tau_{k+1}) &=  z_{k+1}^*(\tau_{k+1}) = s_{k+1}^z,\  k = 0, 1, ..., m-2  
%\end{align*}
%Where $s_{k+1} = (s_{k+1}^y,  s_{k+1}^z) $ are the initial values of $y, z$ at the subinterval $ [\tau_{k+1}, \tau_{k+2}]$, $y_k^*, y_{k+1}^*$ are the solution of $y$ at subinterval  $ [\tau_{k}, \tau_{k+1}], [\tau_{k+1}, \tau_{k+2}]$. Notice, we have $m$ subintervals, therefore $m$ initial values points but only $m-1$ matching conditions. The solution of $y$ at the end of subinterval $ [\tau_{k}, \tau_{k+1}]$ should be equal to the initial value of $y$ of the next subinterval, i.e. $y_k^*(\tau_{k+1}) =  s_{k+1}^y$, the same explanation applies to $z$.  

%The above continuity equations constraints apply to all the subintervals, for simpilicity, we write the solution of $y_k^*, z_k^*$ in interval $ [\tau_{k}, \tau_{k+1}]$ as $s_k^*$, then we have: 
%$$s_k^*(\tau_{k+1}) = s_{k+1} (:= s_{k+1}^*(\tau_{k+1})), k = 0, 1, ..., m-2$$

%The matching condition, together with the IVPs in all the subintervals induce a very specific boundary value problem(BVP) structure. One obvious advantage of multiple shooting is the possibility to bring in a priori information about the state variables, e.g. from the measurements, by a proper choice of initial guesses for the additional variables $s_k$. Thus, it can be shown that the initial values $c(t_i, s_j, p) $ remain close to the measured data. It can also be shown that this damps the influence of the poor parameter guesses on the convergence of (quasi-)Newton methods. Another advantage is the stability of the scheme, which inhibits error propagation and allows to solve parameter estimation problems for unstable and chaotic systems. 

%For our rocket car case, we can apply the multiple shooting method to discretize into subintervals, and enforce the matching condition and the boundary value constraints. This leads to a finite-dimensional, nonlinear equality and/or inequality constrained optimization problems, which can be solved via quasi-Netwon method. In the next chapter, we focus on explain the system structure of the rocket car case employing quasi-Newton and multiple-shooting method.







%There are some other methods which could be used for solving the original problem, e.g. collocation method, which approximates the solution to the original problem in each subinterval by finite linear combination of basis functions(usually polynomials). In each subinterval, the solution of collocation method could be regarded as a local approximation of the original problem solution in the corresponding interval. The matching condition should also be satisfied at each collocation point. 

%Both multiple shooting and collocation methods discretize the original problems into subintervals and have initial guess $s_k, k = 0, 1, ..., m-1$ at discrete time points. In the following part we focus on multiple shooting method and the parallel implementation of such method to solve the original problem. 



% It is given the system of differential equations below
%%\begin{align*}
%\begin{equation}	
%	\dot{x} = f(x(t),t), t_i < t < t_f	
%	\label{shoot_pde}
%\end{equation}
%%\end{align*}
%The initial values $x_i = x(t_i)$ are then determined such that the boundary value $\Phi(x(t_f),t_f)=0$ will give values that satisfy \ref{shoot_pde}. The idea of multiple shooting method is splitting the time domain $t$ into some shorter intervals.


%let $x_k$ for $k = 0, 1, 2, ..., m−1$ be the initial value for the dynamic variables at starting  point of every short interval segment $k + 1$. 

%At each segment $k + 1$, the solution of the differential equations from $t_k$ to $t_{k+1}$ is established, and let the solution be xbk. 



%$x(t_f)-b = 0$ or $x_i e^{t−t_i}-b =0$


%\begin{equation}
%	\begin{aligned}
%		\  \  \ & min \  f(x) \\
%		s.t.\ \  & x \in \Omega
%	\end{aligned}
%	\label{OptGen}
%\end{equation}

% After applying the all the constraints, we can conclude the updating 


% Suppose we have generated a new iterate $x_{k+1}$ and wish to construct a new quadr
%\begin{equation} 
%	A = 
%	\begin{pmatrix}
%		1+2\lambda \theta  & -\lambda \theta  & \cdots & 0 \\
%		-\lambda \theta  & 1+2\lambda \theta & \cdots & 0\\
%		\vdots  & \vdots  & \ddots & \vdots  \\
%		0 & 0 & \cdots & 1+2\lambda \theta
%	\end{pmatrix}
%\end{equation}


%quasi-Newton methods are a class of methods  
%are used to find the global minimum of a function $f(x)$ that is twice-differentiable. 
%We denote by the term quasi-Newton method any modification of
%this scheme employing an approximation Je of the Jacobian J.

